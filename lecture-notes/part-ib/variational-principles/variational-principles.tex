\documentclass[a4paper]{scrartcl}

\usepackage[
fancytheorems, 
fancyproofs, 
noindent, 
%  spacingfix,  
]{adam}


\title{Optimisation}
\author{Adam Kelly (\texttt{ak2316@cam.ac.uk})}
\date{\today}


\allowdisplaybreaks

\begin{document}

\maketitle

At its heart, analysis is the study of ideas that depend on the notion of \emph{limits}.
The main concepts of analysis (such as convergence, continuity, differentiation and integration) will all depend quite fundamentally on a limiting process.

This article constitutes my notes for the `Analysis I' course, held in Lent 2021 at Cambridge. These notes are \emph{not a transcription of the lectures}, and differ significantly in quite a few areas. Still, all lectured material should be covered.

{\color{purple}Currently up to lecture 1.}

\tableofcontents

% \clearpage

This course is roughly split into:
\begin{itemize}
	\item Convex Optimisation (1-3)
	\item General optimisation with Lagrange Multipliers (4-5)
	\item Linear Programming (minimizing linear functions), and applications (6-12)
\end{itemize}

\section{Optimisation Problems}

In optimization, we are interested in solving problems that look like:
minimize $f(x)$ for $x \in X$, or minimize $f(x)$ for $x \in X$, $h(x) = b$ where $h : \R^n \rightarrow \R^m$.

We will always be looking at minimizing functions.

Terminology:
\begin{itemize}
	\item $f$ is called the objective function. 
	\item The components of $x$ are the decision variables.
	\item In the above, `$h(x) = b$' is a functional constraint.
	\item `$x \in X$' is the regional constraint.
	\item The set $\{x \in X : h(x) = b\}$ is called the feasible set $X(b)$. A problem is feasible if this set is not empty, and otherwise it is in-feasible. We say the problem is bounded if the minimum is bounded.
	\item A point $x^* \in X(b)$ is optimal if it minimises $f$ over $X(b)$. The value of $f(x^*)$ is called the optimal constraint.
\end{itemize}

So why do we only care about constraints $h(x) =b$? Well consider the constraint $h(x) \leq b$, then $h(x) + s = b$, $s \geq 0$. So we can always convert an inequality constraint into a functional and regional constraint.

\section{Convex Optimisation}

\begin{definition}[Convex Set]
	A set $S \subseteq \R^n$ is \vocab{convex} if for all $x, y \in S$, and all $\lambda \in [0, 1]$, $x(1 - \lambda) + y(\lambda) \in S$. That is, the line segment joining $x$ and $y$ lies in $S$.
\end{definition}

\begin{center}
	insert diagram 1 - convex here.
\end{center}

We want to study the notion of \emph{convex functions}.

\begin{definition}[Convex Functions]
	A function $f:S \rightarrow \R$ is \vocab{convex} if $S$ is convex, and for all $x, y \in S$, and $\lambda \in [0, 1]$ we have
	$$
f((1 - \lambda)x + \lambda y) \leq (1 - \lambda)f(x) + \lambda f(y).
	$$
	The function is \vocab{strictly convex} if the inequality is strict, and is \vocab{concave} if $-f$ is convex.
\end{definition}

In one dimension, such functions look `$u$-shaped', and points between $x$ and $y$ always lies below the chord.

\subsection{Unconstrained Optimisation}

We are going to look at the problem of minimizing $f(x)$ where $f: \R^n \rightarrow \R$ is a convex function. 

Convex functions have certain properties that allow us to minimize them somewhat easily. The most important property is that \emph{local} information gives us \emph{global} information. Informally, if you know what a convex function looks like in a small neighbourhood, you can always say something about what it looks like outside of that.

Let's have a think about how to tell if a function is convex.

A first order condition for convexity is that a tangent line always lies below the curve. That is,
$$
f(y) \geq f(x) + (y - x) f'(x).
$$
In higher dimensions, we will have to replace $f'(x)$ with $\nabla f(x)$, and take a dot product (treating it as a vector). 
Note that if $\nabla f(x) = 0$, then $f(y) \geq f(x)$, thus $x$ minimizes $f$.

Let's write this all down properly.



\begin{theorem}[First Order Convexity Conditions]
	A differentiable function $f: \R^n \rightarrow \R$ is convex if and only if for $x, y \in \R^n$ we have
	$$
f(y) \geq f(x) + (y - x) \cdot \nabla f(x).
	$$
\end{theorem}
\begin{proof}
	
\end{proof}

\end{document}

\documentclass[a4paper]{scrartcl}

\usepackage[
    fancytheorems, 
    fancyproofs, 
    noindent, 
]{adam}


\title{Complex Analysis}
\author{Adam Kelly (\texttt{ak2316@cam.ac.uk})}
\date{\today}

\allowdisplaybreaks

\begin{document}

\maketitle

% This is a short description of the course. It should give a little flavour of what the course is about, and what will be roughly covered in the notes.

This article constitutes my notes for the `Complex Analysis' course, held in Lent 2022 at Cambridge. These notes are \emph{not a transcription of the lectures}, and differ significantly in quite a few areas. Still, all lectured material should be covered.



\tableofcontents


\section{Basic Notions}

\subsection{Complex Differentiation}

This course is about complex valued functions of a (single) complex variable, that is, functions
$$
f:A \rightarrow \C, \quad \text{where }A \subset C.
$$
In this course we will focus on differentiable functions $f$ (the definition of which we will give soon), but let's start by recalling the notion of continuity.

\begin{definition}[Continuity]
    A function $f: A \rightarrow \C$ is \vocab{continuous} at a point $w \in A$ if for all $\varepsilon > 0$ there exists some $\delta > 0$ such that if $z \in A$ with $|z - w| < \delta$ then $|f(z) - f(w)| < \varepsilon$.
\end{definition}

Note that by identifying $\C$ with $\R^2$ in the usual way, we can write
$
f(z) = u(x, y) + iv(x, y)
$
for $z = x + iy \in A$ with $u, v: A \rightarrow \R$. With this, the triangle inequality implies that $f$ is continuous at $w = c + id$ if and only if $u, v$ are continuous at $(c, d)$.

We can now define the notion of differentiation for complex functions.

\begin{definition}[Complex Differentiation]
    Let $f: U \rightarrow \C$ where $U \subseteq \C$ is open.   
    We say that $f$ is \vocab{differentiable} at $w \in U$ if the limit
    $$
    f'(w) = \lim_{z \to w} \frac{f(z) - f(w)}{z - w}
    $$
    exists as a complex number.
\end{definition}
\begin{definition}[Holomorphic]
    We say that $f$ is \vocab{holomorphic at $w \in U$} if there is $\varepsilon > 0$ such that $D(w, \varepsilon) \subset U$ where $f$ is differentiable at every point in $D(w, \varepsilon)$. We say that $f$ is \vocab{holomorphic} if it is holomorphic at every point in $U$.
\end{definition}

\begin{remark}
    Sometime authors use the word `analytic' instead of the word `holomorphic'.
\end{remark}

\begin{definition}[Entire]
    If $f: \C \rightarrow \C$ is holomorphic on all of $\C$, we say $f$ is \vocab{entire}.
\end{definition}


% for $z = x + iy \in A$ and a pair of real valued functions $u, v : A \rightarrow \R$.

It's straightforward to check that the same rules of differentiation for real variables hold for complex functions (product rule, quotient rule, chain rule, etc), which makes computing complex derivatives relatively straightforward.

\begin{example}[Complex Differentiable Functions]~
    \vspace*{-1.5\baselineskip}
\begin{enumerate}[label=(\roman*)]
    \item Polynomials $p(z) = \sum_{k = 0}^n a_k z^k$, $a_1, \dots, a_k \in \C$ are holomorphic on all of $\C$.
    \item If $p$ and $q$ are polynomials, then $p/q$ is holomorphic on $\C \backslash \{z \mid q(z) = 0\}$.
\end{enumerate}
\end{example}
    
We saw above that the notion of continuity is basically the same for both real and complex variables. A natural question is then if the same is true for complex differentiability.
Indeed, the answer is no! Complex differentiability is a much stronger condition than asserting real differentiability on the real and imaginary components of a function.

We do however have a criterion for establishing complex differentiability from the real and imaginary parts.

\begin{theorem}[Cauchy-Riemann Equations]
    A function $f = u + iv: U \rightarrow \C$ is differentiable at $w = c + id \in U$ if and only if $u, v : U \rightarrow \R$ are differentiable at $(c, d) \in U$ \emph{and} $u, v$ satisfy the \vocab{Cauchy-Riemann} equations at $(c, d)$:
    \begin{align*}
        \frac{\partial u}{\partial x} &= \frac{\partial v}{\partial y} \\
        \frac{\partial u}{\partial y} &= - \frac{\partial v}{\partial x} \quad \text{at }(c, d).
    \end{align*}
    If $f$ is differentiable at $w = c + id$, then $f'(w) = \frac{\partial u}{\partial x}(c, d) + i \frac{\partial v}{\partial x} (c, d)$.
\end{theorem}
\begin{proof}
    The function $f$ is differentiable at $w$ with derivative $f'(w) = p + iq$ if and only if
    $$
    \lim _{z \rightarrow w} \frac{f(z)-f(w)}{z-w}=p+i q \iff \lim _{z \rightarrow w} \frac{f(z)-f(w)-(z-w)(p+i q)}{|z-w|}=0.
    $$
    Separating real and imaginary parts and writing $f = u + iv$, the above holds if and only if
    $$
    \lim _{(x, y) \rightarrow(c, d)} \frac{u(x, y)-u(c, d)-p(x-c)+q(y-d)}{\sqrt{(x-c)^{2}+(y-d)^{2}}}=0
    $$
    and
    $$
    \lim _{(x, y) \rightarrow(c, d)} \frac{v(x, y)-v(c, d)-q(x-c)-p(y-d)}{\sqrt{(x-c)^{2}+(y-d)^{2}}}=0.
    $$
    That is, if and only if $u$ is differentiable at $(c, d)$ with $Du(c, d)(x, y) = px - qy$, and $v$ is differentiable at $(c, d)$ with $Dv(c, d)(x, y) = qx + py$. 

    Taking partial derivatives, this is true if and only if $u, v$ are differentiable at $(c, d)$ and $u_x(c, d) = p = v_y(c, d)$ and $u_y(c, d) = -q = -v_x(c, d)$, that is, the Cauchy-Riemann equations hold at $(c, d)$.

    We also get from the above that if $f$ is differentiable at $w$, then $f'(w) = p + iq = u_x(c, d) + i v_x(c, d)$.
\end{proof}

\begin{remark}[Warning]
    The real and imaginary parts
    $u, v$ satisfying the Cauchy-Riemann equations at a point does \emph{not} guarantee the differentiability of $f$, and a counterexample is given on the first example sheet.
\end{remark}

We can easily use this criterion to show that many functions are not complex differentiable.

\begin{example}[Conjugation isn't Differentiable]
    Consider the function $f(z) = \overline{z} = x - iy$. For this, $u(x, y) = x$ and $v(x, y) = -y$, so $u_x = 1$ and $u_y = 0$, $v_x = 0$ and $v_y = -1$. So the Cauchy-Riemann equations do not hold anywhere, and $f$ is not differentiable at any point.
\end{example}

Now, if we just wanted to show that the differentiability of $f = u + iv$ at $w = c + id$ implies that the partial derivatives $u_x, u_y, v_x$ and $v_y$ exist at $(c, d)$ and satisfy the Cauchy-Riemann equations, then we can proceed more simply as follows. 

Begin with the definition of differentiability
$$
f'(w) = \lim_{h \to 0} \frac{f(w + h) - f(w)}{h}.
$$
Then taking $h = t \in \R$, we get
$$
f'(w) = \lim_{t \to 0} \left(
\frac{c(u + t, d) - u(c, d)}{t} + i \frac{v(c + t, d) - v(c, d)}{t}
\right),
$$
from which the real and imaginary parts imply that the limits
$$
\lim _{t \rightarrow 0} \frac{u(c+t, d)-u(c, d)}{t} \quad \text{and} \quad \lim_{t \rightarrow 0} \frac{v(c+t, d)-v(c, d)}{t}
$$
both exist, that is, $u_x(c, d)$ and $v_x(c, d)$ exists, and $f'(w) = u_x(c, d) + i v_x(c, d)$. 
Similarly, taking $h = it$ with $t \in \R$ gives $f'(w) = v_y(c, d) - i u_y(c, d)$, so $u_x = v_y$ and $u_y = -v_x$ at $(c, d)$.


Because partial derivatives are a little easier to work with than the standard derivative of a function on $\R^2$, we can use a straightforward corollary of the Cauchy-Riemann equations for checking complex differentiability\footnote{Remarkably, the \emph{Looman-Menchoff} theorem says that it suffices for $u$ and $v$ to be continuous and satisfy the Cauchy-Riemann equations to achieve complex differentiability, but this is quite non-trivial to prove.}.

\begin{corollary}[Cauchy-Riemann and Continuous Partials implies Holomorphic]
    Let $f = u + iv: U \rightarrow \C$. If $u, v$ have continuous partial derivatives at a point $(c ,d) \in U$ and satisfy the Cauchy-Riemann equations at $(c, d)$, then $f$ is differentiable at $w = c + id$.
    % In particular, if $u, v$ are $C^1$ functions on $U$ satisfying the Cauchy-Riemann equations in $U$, then $f$ is holomorphic in $U$.
\end{corollary}
\begin{proof}
    Continuity of partial derivatives of $u$ implies that $u$ is differentiable, and similarly for $v$. So this follows from our original theorem.
\end{proof}

Complex differentiability is a \emph{much} stronger notion than the differentiability of real and imagine parts. This leads to some surprising theorems compared to the real case. For example: 
\begin{enumerate}[label=(\alph*)]
    \item \emph{Liouville's Theorem}. If $f: \C \rightarrow \C$ is holomorphic and bounded then $f$ is constant.
    \item If $f: U \rightarrow \C$ is holomorphic, then $f$ is automatically infinitely differentiable on $U$.
\end{enumerate}
We will prove these and much more later on.

\begin{remark}[Harmonic Functions]
That the second theorem implies that partial derivatives of $u, v$ exist to all orders, so we can differentiate the Cauchy-Riemann equations to get
$$
u_{xx} = v_{yx}, \quad \text{and} \quad u_{uu} = -v_{xy},
$$
and since $v_{yx} = v_{xy}$, we get
$$
\nabla^2 u = u_{xx} + u_{yy} = 0 \quad \text{in }U, 
$$
and similarly $\nabla^2 v = 0$ in $U$. So \emph{real and imaginary parts of a holomorphic function are harmonic}.
\end{remark}

We will now consider curves in the complex plane.

\begin{definition}[Curve]
A \vocab{curve} is a continuous map $\gamma: [a, b] \rightarrow \C$, where $[a, b] \subset \R$ is a closed interval. We say $\gamma$ is a \vocab{$C^1$ curve} if $\gamma'$ exists and is continuous on $[a, b]$. 
\end{definition}

\begin{definition}[Path Connected]
    An open set $U \subset \C$ is \vocab{path connected} if for any two points $z, w \in U$ there is a curve $\gamma : [0, 1] \rightarrow U$ such that $\gamma(0) = z$ and $\gamma(1) = w$.
\end{definition}

\begin{definition}[Domain]
    A \vocab{domain} is a non-empty, open, path connected subset of $\C$.
\end{definition}

\begin{corollary}[Zero Derivative implies Constant]
    If $U \subset \C$ is a domain and $f: U \rightarrow \C$ is holomorphic with $f'(z) = 0$ for every $z \in U$, then $f$ is constant.
\end{corollary}
\begin{proof}
    Write $f = u + iv$. By the Cauchy-Riemann equations, $f' = 0$ implies that $Du = 0$ and $Dv = 0$ in $U$. Then since $U$ is a domain, we get that $u$ and $v$ are constant.
\end{proof}

\subsection{Power Series}

So far we've only seen very few explicit holomorphic functions (namely polynomials on $\C$ and rational functions on their domains). We'd like to generate more, which we can do by looking at \emph{power series}.

Recall the notion of \emph{radius of convergence}.

\begin{theorem}[Radius of Convergence]
    If $c_n \in \C$ is a sequence of complex numbers, then there is a unique $R \in [0, \infty]$, the \vocab{radius of convergence}, such that the power series
    $$
\sum_{n = 0}^{\infty} c_n(z - a)^n, \quad z, a \in \C
    $$
    converges absolutely if $|z - a| < R$ and diverges if $|z - a| > R$. If $0 < r < R$, then the series converges uniformly (with respect to $z$) on the compact disk $\overline{D(a, r)} = \{z \in \C : |z - a| \leq r\}$.
\end{theorem}

Note that there are no claims about convergence when $|z - a| = R$ for $R > 0$. There are also various expressions for $R$, for example
$$
R = \sup \{r \geq 0 \mid |c_n|r^n \rightarrow 0 \text{ as } n \rightarrow \infty \}
$$
and
$$
\frac{1}{R} = \limsup_{n \to \infty} |c_n|^{1/n}.
$$

We get quite natural behavior of power series with respect to complex differentiability.

\begin{theorem}[Derivatives of Power Series]
    Let $\sum_{n = 0}^{\infty} c_n(z - a)^n$ be a power series with radius of convergence $R > 0$. Fix some $a \in \C$, and define $f: D(a, R) \rightarrow \C$ by $f(z) = \sum_{n = 0}^{\infty} c_n(z - a)^n$. Then
    \begin{enumerate}[label=(\roman*)]
        \item $f$ is holomorphic on $D(a, R)$;
        \item the derived series $\sum_{n = 1}^{\infty} n c_n (z - a)^{n - 1}$ also has radius convergence $R$, and is equal to $f'(z)$ for $z \in D(a, R)$.
        \item $f$ has derivatives of all orders on $D(a, R)$, and $c_n = \frac{f^{(n)}(a)}{n!}$.
        \item if $f$ vanishes on $D(a, \varepsilon)$ for some $\varepsilon > 0$, then $f \equiv 0$ on $D(a, R)$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \emph{Parts (i) and (ii)}. Without loss of generality, take $a = 0$. Now we have $f(z) = \sum_{n = 0}^{\infty} c_n z^n$ for $z \in D(0, R)$, where $R$ is the radius of convergence.

    The derived series $\sum_{n = 1}^{\infty} n c_n z^{n - 1}$ will have some radius of convergence $R_1 in [0, \infty]$. To see that $R_1 \geq R$, let $z \in D(0, R)$, and choose $\rho$ such that $|z| < \rho < R$. Then
    $$
n|c_n||z|^{n - 1} = n |c_n| \left|\frac{z}{\rho}\right|^{n - 1}\rho^{n - 1} \leq |c_n|\rho^{n - 1}
    $$
    for sufficiently large $n$ (as $n |z/\rho|^{n - 1} \rightarrow 0$ as $n \rightarrow \infty$). Since $\sum |c_n| \rho^n$ converges, it follows that $\sum_{n = 1}^{\infty} n|c_n||z|^{n - 1}$ converges. Thus $D(0, R) \subset D(0, R_1)$ that is, $R_1 \geq R$.

    Also, since $|c_n||z|^n \leq n |c_n| z^n = |z|(n |c_n||z|^{n - 1})$, if $\sum n |c_n||z|^{n - 1}$ converges then so does $\sum |c_n||z|^n$, so $R_1 \leq R$. This implies that we must have $R_1 = R$.

    To prove that $f$ is differentiable with $f'(z) = \sum_{n = 1}^{\infty} n c_n z^{n - 1}$, fix some $z \in D(0, R)$. Then our key is idea is that this assertion is equivalent to the continuity at $z$ of the function $g: D(0, R) \rightarrow \C$ with
    $$
    g(w) =\begin{cases}
        \frac{f(w) -f(z)}{w - z} &\mbox{if } w \neq z, \\
        \sum_{n = 1}^{\infty} n c_n z^{n - 1} &\mbox{if } w = z.
       \end{cases}
    $$
    By substituting for $f$, we get that $g(w) = \sum_{n = 1}^{\infty} h_n(w)$ for $w \in D(0, R)$ where $h_n$ is a sequence of functions given by
    $$
h_n(w) = \begin{cases}
    \frac{c_n(w^n - z^n)}{} &\mbox{if } w \neq z, \\
    n c_n z^{n - 1} &\mbox{if } w = z.
   \end{cases}
    $$
    Now $h_n$ is continuous on $D(0, R)$ for all $n$, then using that $\frac{w^n - z^n}{w - z} = \sum_{j = 0}^{n - 1} z^j w^{n - 1 - j}$, we get that for any $r$ with $|z| < r < R$ and any $w \in D(0, r)$ we have $|h_n(w)| \leq n|c_n|r^{n - 1}$. Defining $M_n = n |c_n|r^{n - 1}$, we can note that $\sum M_n < \infty$, so by the Weierstrass M-test, $\sum h_n$ converges uniformly on $D(0, r)$. But a uniform limit of continuous functions is continuous, so $g = \sum h_n$ is continuous in $D(0, r)$ and in particular at $z$, as required.

    \emph{Part (iii)}. Repeatedly apply (ii), and the formula $c_n = f^{n}(a)/n!$ follows by differentiating the series $n$ times and setting $z = a$.

    \emph{Part (iv)}. If $f = 0$ on $D(a, \varepsilon)$, then $f^{(n)}(a) = 0$ for all $n$, so $c_n = 0$ for all $n$. Hence $f = 0$ on $D(a, R)$.
\end{proof}


\begin{remark}
    This theorem provides a way to generate a large class of holomorphic functions on a disk. Later we will show that every holomorphic function is \emph{locally} given by a power series. Once we have that, (iii) above gives that holomorphic functions are automatically infinitely differentiable in their domain.
\end{remark}

Let's now consider a specific example of power series, the \emph{complex exponential} function, which you are likely familar with.

\begin{definition}[Complex Exponential]
    The \vocab{complex exponential} function is defined by the power series
    $$
    e^z = \exp(z) = \sum_{n = 0}^{\infty} \frac{z^n}{n!}.
    $$
\end{definition}

Some basic properties of this function will also be familiar.

\begin{proposition}[Properties of the Exponential]~
        \vspace*{-1.5\baselineskip}
    \begin{enumerate}[label=(\roman*)]
        \item $e^z$ is entire, with $(e^z)' = e^z$.
        \item $e^z \neq 0$ and $e^{z + w} = e^z  \cdot e^w$ for all $z, w \in \C$.
        \item $e^{x + iy} = e^x(\cos y + i \sin y)$ for $x, y \in \R$.
        \item $e^z = 1$ if and only if $z = 2\pi n i$ for some integer $n$.
        \item Let $z \in \C$. Then there exists $w \in \C$ such that $e^w = z$ if and only if $z \neq 0$.
    \end{enumerate}
\end{proposition}
\begin{proof}
    \emph{Part (i)}. The radius of convergence of the series is $\infty$, and to see that $(e^z)' = e^z$, differentiate the series term-by-term.

    \emph{Part (ii)}. Fix any $w \in \C$ and set $F(z) = e^{z + w} \cdot e^{-z}$. Then $F'(z) = - e^{z + w} \cdot e^{-z} + e^{z + w} \cdot e^{-z} = 0$, so $F(z)$ is constant, and equal to $e^w$. Thus $e^{z + w} \cdot e^{-z} = e^w$ for all $z, w \in \C$, and the result follows noting that $e^0 = 1$.

    \emph{Part (iii)}. By the previous part, $e^{x + iy} = e^x \cdot e^{iy}$. Now use the definition of $e^{iy}$ and the series for $\sin y$ and $\cos y$ with $y \in \R$.

    \emph{Part (iv) and (v)}. Both follow directly from (iii).
\end{proof}

We can now consider the `inverse' of this function, the \emph{logarithm}.

\begin{definition}[Logarithm]
    Given $z \in \C$, we say that $w \in \C$ is a \vocab{logarithm} of $z$ if $e^w = z$.
\end{definition}

By our above theorem, we note that $z$ has a logarithm if and only if $z \neq 0$, and also if $z$ \emph{does} have a logarithm then there is infinitely many such logarithms, each differing from each other by $2 \pi n i$ for some integer $n$.

If $w$ is a logarithm of $z$, then $e^{\re(w)} = |z|$, so $\re(w) = \ln |z|$ (the real logarithm of the positive number $|z|$). In particular, $\re(w)$ is uniquely determined by $z$.

\begin{definition}[Branches of the Logarithm]
    Let $U \subset \C \backslash \{0\}$ be open. Then a \vocab{branch} of the logarithm on $U$ is a continuous function $\lambda : U \rightarrow \C$ such that $e^{\lambda(z)} = z$ for each $z \in U$.
\end{definition}

\begin{remark}
    If $\lambda$ is a branch of $\log$ on $U$, then $\lambda$ is automatically holomorphic in $U$, with $\lambda'(z) = 1/z$, with the proof following by direct computation of limits.
\end{remark}

\begin{definition}[Principal Branch of the Logarithm]
The \vocab{principal branch} of the logarithm is the function
$$
\operatorname{Log}: U_1 = \C \backslash \{x \in \R : x \leq 0\} \rightarrow \C
$$
defined by
$$
\operatorname{Log}(z) = \ln|z| + i \arg(z),
$$
where $\arg(z)$ is the unique argument of $z \in U_1$ in $(-\pi, \pi)$.
\end{definition}

\begin{remark}
    $\operatorname{Log}$ does not have a continuous extension to $\C \backslash \{0\}$, since $\arg(z) \rightarrow \pi$ as $z \rightarrow -1$ with $\im(z) > 0$, and $\arg(z) \rightarrow -\pi$ as $z \rightarrow -1$ with $\im (z) < 0$. We will later see that it is impossible to define a branch of $\log$ on $\C \backslash \{0\}$.
\end{remark}

\begin{proposition}[Power Series for $\operatorname{Log}$]
    For $|z| < 1$, we have
    $$
    \operatorname{Log}(1 + z)=\sum_{n=1}^{\infty} \frac{(-1)^{n-1} z^{n}}{n}.
    $$
\end{proposition}
\begin{proof}
    Note that the radius of convergence of the series is $1$ and $|z| < 1$ implies that $1 + z \in U_1$, so both sides are defined on $|z| < 1$. Then let $F(z) = \operatorname{Log}(1 + z) - \sum_{n=1}^{\infty} \frac{(-1)^{n-1} z^{n}}{n}$, for $|z| < 1$. Then $F'(z) = \frac{1}{1 + z} - \sum_{n = 1}^{\infty} (-z)^{n - 1} = 0$. So $F(z)$ is constant, and $F(z) = F(0) = 0$.
\end{proof}

Using $\exp$ and $\operatorname{Log}$ we can define further useful functions.

\begin{enumerate}[label=(\roman*)]
    \item For any constant $\alpha \in \C$, define
    $$
    z^{\alpha} = e^{\alpha \operatorname{Log}(z)}, \quad z \in U_1.
    $$
    This is the principal branch of $z^\alpha$. We have that $z^{\alpha}$ is holomorphic on $U_1$ with $(z^{\alpha})' = \alpha z^{\alpha - 1}$.

    \item We can define
    \begin{align*}
        \cos(z) &= \frac{e^{iz} + e^{-iz}}{2}\\ 
        \sin(z) &= \frac{e^{iz} - e^{-iz}}{2i}\\
        \cosh(z) &= \frac{e^{z} + e^{-z}}{2} \\
        \sinh(z) &= \frac{e^z - e^{-z}}{2}.
    \end{align*}
    All of these functions are entire and their derivatives are given by the familiar expressions from real variables.
\end{enumerate}

\subsection{Conformal Maps}

We now look at a geometric property of holomorphic functions called \emph{conformality}.
let $f: U \rightarrow \C$ be holomorphic, where $U \subset \C$ is open, as usual. Let $w \in U$, and suppose that $f'(w) \neq 0$.

Take two $C^1$ curves $\gamma_1, \gamma_2 : [-1, 1] \rightarrow U$, such that $\gamma_i(0) = w$ and $\gamma_i'(0) \neq 0$. Then $f \circ \gamma_i$ are $C^1$ curves passing through $f(w)$. 
Moreover, $(f \circ \gamma_i)'(0) = f'(w) \gamma_i'(0) \neq 0$.

This implies that
$$
\frac{(f \circ \gamma_1)'(0)}{(f \circ \gamma_2)'(0)} = \frac{\gamma_1'(0)}{\gamma_2'(0)},
$$
and hence
$$
\arg(f \circ \gamma_1)'(0) - \arg(f \circ \gamma_2)'(0) = \arg \gamma_1'(0) - \arg \gamma_2'(0).
$$

This means that the angle that the curves $\gamma_1, \gamma_2$ make at $w$ is the same as the angle their images $f \circ \gamma_1$ and $f\circ \gamma_2$ make at $f(w)$, in size as well as in orientation. That is, $f$ is `angle-preserving at $w$' whenever $f'(w) \neq 0$. 

\begin{remark}
    If $f$ is a $C^1$ map on $U$, the converge of the above also holds. That is, if $w \in U$ and $f$ has the property such that $(f \circ \gamma)'(0) \neq 0$ for any $C^1$ curve $\gamma$ with $\gamma(0) = w$ and $\gamma'(0) \neq 0$, and if $g$ is angle preserving at $w$ in the above sense, then $f'(w)$ exists and $f'(w) \neq 0$.
\end{remark}

\begin{definition}[Conformal]
    A holomorphic function $f: U \rightarrow \C$ on an open set $U$ is said to be \vocab{conformal} at the point $w \in U$ if $f'(w) \neq 0$.
\end{definition}

\begin{definition}[Confirmal Equivalence]
    Let $U, \tilde{U}$ be domains in $\C$. A map $f: U \rightarrow \tilde{U}$ is said to be a \vocab{confirmal equivalence} between $U$ and $\tilde{U}$ if $f$ is a bijective holomorphic map with $f'(z) \neq 0$ for every $z \in U$.
\end{definition}

\begin{remark}[Redundancy]
    If $f$ is holomorphic and injective, then $f'(z) \neq 0$ for each $z$ (we will see this later in the course). So, in the above definition, the requirement $f'(z) \neq 0$ is redundant.
\end{remark}

\begin{remark}[Inverse Functions]
    It is automatic that the inverse $f^{-1}: \tilde{U} \rightarrow U$ is holomorphic. This follows from the \vocab{holomorphic inverse function theorem}, which can be proved using the real inverse function theorem.
\end{remark}

We will now consider some examples of conformal equivalences.

\begin{example}[Möbius Maps Give a Conformal Equivalence]
    Consider the Möbius maps
    $$
f(z) = \frac{az + b}{cz + d}
    $$
    where $a, b, c, d \in \C$ with $ad - bc \neq 0$. This function $f$ is conformal on $\C \backslash \{-d/c\}$ if $c \neq 0$, and conformal on $\C$ if $c \neq 0$.

    Möbius maps sometimes serve as explicit conformal equivalences between subdomains of $\C$.

    For example, let $\mathbb{H}$ be the open upper-half plane, $\mathbb{H} = \{z \mid \im(z) > 0\} \cap \C$. Then
    \begin{align*}
        z \in \mathbb{H} &\iff z \text{ is closer to $i$ than to $-i$} \\
&\iff |z - i| < |z + i| \\
&\iff \left|\frac{z - i}{z + i}\right| < 1.
    \end{align*}
    Thus $g = \frac{z - i}{z + i}$ maps $\mathbb{H}$ onto $D(0, 1)$, so $g: \mathbb{H} \rightarrow D(0, 1)$ is a conformal equivalence.
\end{example}

\begin{example}[Powers Give a Conformal Equivalence]
    Let $f(z) = z^n$ where $n \in \N$, with
    $$
    f: \{z \in C \backslash \{0\} \mid 0 < \arg(z) < \pi/n\} \rightarrow \mathbb{H}.
    $$
    This is a conformal equivalence, with inverse $f^{-1}(z) = z^{1/n}$.
\end{example}

\begin{example}[Exponentials Give a Conformal Equivalence]
The restricted exponential function given by
$$
\exp: \{z \in \C \mid -\pi < \im (z) < \pi\} \rightarrow \C \backslash \{x \in \R \mid x \leq 0\}
$$
is a conformal equivalence, whose inverse is $\operatorname{Log}(z)$.
\end{example}


\begin{aside}{Aside: The Riemann Mapping Theorem}

    There is an interesting result about many domains being conformally equivalent.
    
    \begin{theorem}[Riemann Mapping Theorem]        
        Any simply connected domain $U \subset \C$ with $U \neq \C$ is conformally equivalent to $D(0, 1)$.
    \end{theorem}

    Here, $U \subset \C$ is simply connected if every continuous map $\gamma: S^1 = \partial D(0, 1) \rightarrow U$ extends to a continuous map $\Gamma: \overline{D(0, 1)} \rightarrow U$ with $\left.\Gamma\right|_{\partial D(0, 1)} = \gamma$.
    We will discuss simply connected domains in detail later on, but intuitively one should think of simply connected domains as the ones `without holes'.
    
    A proof of this result can be found in many textbooks on complex analysis.

\end{aside}

\section{Complex Integration}

\subsection{Extending Riemann Integration}

As we did with differentiation, we now want to extend our real (Riemann) notion of integration to the integration of complex functions $f: U \rightarrow \C$ with $U \subset \C$ along curves in $U$.

First we will look at complex functions of a single real variable.

\begin{definition}[Integral on a Real Interval]
    If $f: [a, b] \subset \R \rightarrow \C$ is a complex function and if $f$ is continuous\footnote{Or, more generally, $f$ is Riemann integrable in it's real and imaginary parts.} then we define the \vocab{integral} of $f$ over $[a, b]$ to be
    $$
    \int_a^b f(t) \dd t = \int_a^b \re(f(t)) \dd t + i \int_a^b \im(f(t)) \dd t.
    $$
\end{definition}

In particular,
    $$
    \int_a^b i g(t) \dd t = i \int_a^b g(t) \dd t
    $$
    for a real function $g: [a, b] \rightarrow \R$. Hence, by direct calculation,
    $$
\int_a^b w f(t) \dd t = w \int_a^b f(t) \dd t
    $$
    for any $w \in \C$.

\begin{proposition}[Basic Estimate]
    If $f: [a, b] \rightarrow \C$ is continuous then
    $$
    \left|\int_{a}^{b} f(t) d t\right| \leq \int_{a}^{b}|f(t)| \dd t \leq(b-a) \sup _{t \in[a, b]}|f(t)|
    $$
    with equality if and only if $f$ is constant.
\end{proposition}
\begin{proof}
    If $\int_a^b f(t) \dd t = 0$ then we are done. Otherwise, write $\int_a^b f(t) \dd t = re^{i \theta}$, with $\theta \in [0, 2\pi)$. Let $M = \sup_{t \in [a, b]} |f(t)|$. Then
    \begin{align*}
        \left|\int_a^b f(t) \dd t\right| &= r = e^{-i \theta} \int_a^b f(t) \dd t = \int_a^b e^{-i \theta} f(t) \dd t \\
        &= \int_a^b \re(e^{-i \theta} f(t)) \dd t + i \int_a^b \im(e^{- i \theta} f(t)) \dd t.
    \end{align*}
    Since the left-hand-side is real,
    \begin{align*}
        \left|\int_{a}^{b} f(t) \dd t\right|&=\int_{a}^{b} \operatorname{Re}\left(e^{-i \theta} f(t)\right) \dd t \\
        &\leq \int_{a}^{b}\left|e^{-i \theta} f(t)\right| \dd t \\
        &= \int_a^b |f(t)| \dd t \\
        & \leq (b - a) M.
    \end{align*}
    Equality holds if and only if $|f(t)|=M$ and $\re\left(e^{-i \theta} f(t)\right)=M$ for all $t \in[a, b]$, that is, if and only if $|f(t)|=M$ and $\arg (f(t))=\theta$ for all $t \in[a, b]$, that is, $f$ is constant.
\end{proof}

We can now define a more general notion of an integral along a curve in the complex plane (not just somewhere on the real line).

\begin{definition}[Integral on a Complex Curve]
    Let $U \subset \C$ be open and $f: U \rightarrow \C$ be continuous. Let $\gamma: [a, b]  \rightarrow U$ be a $C^1$ curve. Then the \vocab{integral of $f$ along $\gamma$} is
    $$
    f_{\gamma} f(z) \dd z = \int_a^b f(\gamma(t)) \gamma'(t) \dd t.
    $$
\end{definition}

This integral has lots of nice basic properties that one might expect.

\begin{proposition}[Invariance under Reparameterisation]
Let $\varphi:\left[a_{1}, b_{1}\right] \rightarrow[a, b]$ be $C^{1}$ and injective with $\varphi\left(a_{1}\right)=a$ and $\varphi\left(b_{1}\right)=b$. Let
$\delta=\gamma \circ \varphi:\left[a_{1}, b_{1}\right] \rightarrow U$. Then we have
$$
\int_{\delta} f(z) \dd z=\int_{\gamma} f(z) \dd z
$$
\end{proposition}
\begin{proof}
    Letting $s = \phi(t)$, we have
    \begin{align*}
\int_{\delta} f(z) \dd z&=\int_{a_{1}}^{b_{1}} f(\gamma \circ \varphi(t)) \gamma^{\prime}(\varphi(t)) \varphi^{\prime}(t) \dd t \\
&= \int_{a}^{b} f(\gamma(s)) \gamma^{\prime}(s) \dd s \\ &=\int_{\gamma} f(z) \dd z.
    \end{align*}
    The second equality follows from the definition of the integral of a function of a real variable and the change of variables formula for integrals of real functions of a real variable.
\end{proof}

\begin{proposition}[Linearity]
    We have
    $$
    \int_{\gamma}\left(c_{1} f_{1}(z)+c_{2} f_{2}(z)\right) \dd z=c_{1} \int_{\gamma} f_{1}(z) \dd z+c_{2} \int_{\gamma} f_{2}(z) \dd z,
    $$
    for constants $c_1, c_2 \in \C$.
\end{proposition}
\begin{proof}[Proof Sketch]    
    Check definitions.
\end{proof}

\begin{proposition}[Additivity]
    If $\gamma:[a, b] \rightarrow U$ is a $C^{1}$ curve and $a<c<b$, then 
    $$\int_{\gamma} f(z) \dd z=\int_{\left.\gamma\right|_{[a, c]}} f(z) \dd z+\int_{\left.\gamma\right|_{[c, b]}} f(z) \dd z .
    $$
\end{proposition}
\begin{proof}[Proof Sketch]    
    Check definitions.
\end{proof}

\begin{proposition}[Inverse Path]
Define the inverse path $(-\gamma):[-b,-a] \rightarrow U$ by $(-\gamma)(t)=\gamma(-t)$ for $-b \leq t \leq-a$. Then
$$
\int_{(-\gamma)} f(z) \dd z=-\int_{\gamma} f(z) \dd z .
$$
\end{proposition}
\begin{proof}[Proof Sketch]    
    Check definitions.
\end{proof}

We can get the euclidean length of a curve in the complex plane by the following (which we take as a definition).

\begin{definition}[Length of a Curve]
Let $\gamma:[a, b] \rightarrow \mathbb{C}$ be a $C^{1}$ curve. The \vocab{length} of $\gamma$ is defined by
$$
\operatorname{length}(\gamma)=\int_{a}^{b}\left|\gamma^{\prime}(t)\right| \dd t
$$
\end{definition}

\begin{definition}[Piecewise $C^1$ Curve]
    A \vocab{piecewise $C^1$ curve} is a continuous map $\gamma: [a, b] \rightarrow \C$ such that there exists a finite subdivision
    $a = a_0 < a_1 < \cdots < a_n = b$ with the property that $\gamma_j = \left.\gamma\right|_{[a_{j - 1}, a_j]}: [a_{j - 1}, a_j] \rightarrow \C$ is $C^1$ for $1 \leq j \leq n$.
\end{definition}

If $\gamma$ is a piecewise $C^1$ curve as above, we \emph{define}
$$
\int_\gamma f(z) \dd z = \sum_{j = 1}^n \int_{\gamma_j} f(z) \dd z
$$
and
$$
\operatorname{length}(\gamma) = \sum_{j = 1}^{n} \operatorname{length}(\gamma_j) = \sum_{j = 1}^n \int_{a_{j - 1}}^{a_j} |\gamma'(t)| \dd t.
$$

\begin{remark}
    Note that both definitions are independent of the subdivisions by the additivity property above.
\end{remark}

With this, from now on a `curve' will mean a piecewise $C^1$ curve.

\begin{definition}[Sum of Curves]
    If $\gamma_1: [a, b] \rightarrow \C$ and $\gamma_2 : [c, d] \rightarrow \C$ are curves with $\gamma_1(b) = \gamma_2(c)$, we define the \vocab{sum} of $\gamma_1$ and $\gamma_2$ to be the curve $(\gamma_1 + \gamma_2): [a, b + d - c] \rightarrow \C$, with
    $$
    (\gamma_1 + \gamma_2)(t) = \begin{cases}
        \gamma_1(t) &\mbox{if } a \leq t \leq b, \\
        \gamma_2(t - b + c) &\mbox{if } b \leq t \leq b + d - c.
       \end{cases}
    $$
\end{definition}

We can then give another basic estimate for integrals over curves in the complex plane.

\begin{proposition}[Another Basic Estimate]
    For any continuous function $f: U \rightarrow \mathbb{C}$ and any curve $\gamma:[a, b] \rightarrow \mathbb{C}$, we have that
    $$
    \left|\int_{\gamma} f(z) \dd z\right| \leq \operatorname{length}(\gamma) \sup _{\gamma}|f|
    $$
    where $\sup _{\gamma}|f|=\sup _{t \in[a, b]}|f(\gamma(t))|$.
\end{proposition}
\begin{proof}
If $\gamma$ is $C^{1}$, then 
\begin{align*}
\left|\int_{\gamma} f(z) \dd z\right|&=\left|\int_{a}^{b} f(\gamma(t)) \gamma^{\prime}(t) \dd t\right| \\
&\leq\int_{a}^{b}\left|f(\gamma(t)) \| \gamma^{\prime}(t)\right| \dd t \\
&\leq \sup _{t \in[a, b]}|f(\gamma(t))| \operatorname{length}(\gamma).
\end{align*} 

If $\gamma$ is piecewise $C^{1}$ then the result follows from the definition 
$$\int_{\gamma} f(z) \dd z=\sum_{j=1}^{n} \int_{\gamma_{j}} f(z) \dd z$$ 
where $\gamma_{j}$ is $C^{1}$, and the triangle inequality.
\end{proof}

\section{BREAK UNTIL LECTURE 9}

\section{Uniform Limit of Holomorphic Functions}

We now look at sequences of holomorphic functions that converge uniformly.

\begin{definition}[Locally Uniform Convergence]
    Let $U \subset \C$ be open, and let $f_n : U \rightarrow \C$ be a sequence 
    of functions. We say that $f_n$ converges \vocab{locally uniformly} on $U$ if for each $a \in U$ there is some $r > 0$ such that $f_n$ converges uniformly on $D(a, r)$.
\end{definition}

\begin{example}[Sequence of Functions Converging Locally Uniformly]
    Consider the sequence of functions $f_n(z) = z^n$. Then $f_n \rightarrow 0$ locally uniformly on $D(0, 1)$, but not uniformly on $D(0, 1)$. This is because $f_n \rightarrow 0$ uniformly on any disk $D(0, r)$ if $r < 1$, but $\sup_{z \in D(0, 1)} |f_n(z)| = 1$ for each $n$.
\end{example}

\begin{proposition}
    The sequence of functions $f_n$ converges locally uniformly on $U$ if and only if $f_n$ converges uniformly on each compact subset $K \subset U$.
\end{proposition}
\begin{proof}
    The forward direction is a straightforward application of the definition of compactness. The other direction is clear since for all $a \in U$, there's a compact disk $\overline{D(a, r)} \subset U$.

    {\color{red} Fill in details}
\end{proof}

\begin{theorem}[Uniform Limits of Holomorphic Functions]
    Let $U \subset \C$ be open, and $f_n: U \rightarrow \C$ be a sequence of holomorphic functions. If $f_n$ converges locally uniformly on $U$ to some function $f: U \rightarrow \C$, then $f$ is holomorphic. Moreover, $f'_n \rightarrow f'$ locally uniformly on $U$\footnote{This result, by applying it iteratively to get $f_n^{(k)} \rightarrow f^{(k)}$ locally uniformly on $U$.}.
\end{theorem}

This result spectacularly fails for real analytic functions, as seen by the following theorem:

\begin{theorem}[Weierstrass Approximation Theorem]
    Let $f: [a, b] \rightarrow \R$ be a continuous function on a compact interval $[a, b] \subset \R$. Then there is a sequence of polynomials.
\end{theorem}

\end{document}

%  \documentclass[DIV=12, a4]{scrartcl}
%\documentclass[12pt, a5]{scrartcl}

% \documentclass[a4paper]{report}
% \usepackage[
% % fancytheorems, 
% noindent, 
% %spacingfix, 
% %noheader
% ]{vanilla}


\documentclass[a4paper]{scrreprt}
\usepackage[
fancytheorems, 
noindent, 
% %spacingfix, 
% %noheader,
fancyproofs
]{adam} 

\usepackage{tikz}

% \usepackage{subfig}

% \setcounter{chapter}{-1}

\title{Probability}
% \subtitle{Adam Kelly}
\author{Adam Kelly}
% \date{Michaelmas 2020}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
	
	% \vspace{2\baselineskip}
	% {\color{red} None of the notes here have been reviewed at all, and are just exactly what was taken down live in the lectures. I would turn around now and come back in a few days, when I have gone back, cleaned things up, fixed explanations and added some structure.}
	% \vspace{5\baselineskip}

	This set of notes is a work-in-progress account of the course `Probability', originally lectured by Dr Perla Sousi in Lent 2020 at Cambridge. These notes are not a transcription of the lectures, but they do roughly follow what was lectured (in content and in structure).

	These notes are my own view of what was taught, and should be somewhat of a superset of what was actually taught. I frequently provide different explanations, proofs, examples, and so on in areas where I feel they are helpful. Because of this, this work is likely to contain errors, which you may assume are my own. If you spot any or have any other feedback, I can be contacted at \href{mailto:ak2316@cam.ac.uk}{ak2316@cam.ac.uk}.


	% {\color{red} Notes written upto lecture 6.}
	% During the creation of this document, I consulted a number of other books and resources. All of these are listed in the bibliography. 

\end{abstract}

\tableofcontents

\clearpage

\chapter{Basic Concepts}

Most of the phenomena in everyday lives involve randomness. What we try to do in probability is model this randomness in a mathematical way. It's likely that you have studied some probability before, but the difference in the treatment here is that we will try to be somewhat more rigerous.

We will define the notion of a probability space, where `our experiments take place'. Then we will discuss discrete and continuous random variables. 
In the discrete setting, we will find that there is no real subtleties, and we can be quite rigorous. In the continuous setting however we will have to take some things for granted (but rigour will return in the Part II course).

\begin{quote}
	\emph{``Probability theory has a right and a left hand. On the right is the rigorous foundational work using the tools of measure theory. The left hand `thinks probabilistically,' reduces problems to gambling situations, coin-tossing, motions of a physical particle.''}
\end{quote}

In this course, we will need both hands.

\section{Probability Space}


Probability is the mathematical formulation of randomness. So in order to study random phenomena in a rigorous way, we first need to set out a rigorous mathematical framework. 

The first notion that we will define is that of a \emph{probability space}.

\begin{definition}[$\sigma$-Algebra]
Suppose $\Omega$ is a set and $\FF$ is a collection of subsets of $\Omega$. We call $\FF$ a \vocab{$\sigma$-algebra} if the following properties are satisfied.
\begin{enumerate}[label=(\roman*)]
	\item $\Omega \in \FF$.
	\item If $A \in \FF$, then $A^c \in \FF$, the compliment of $A$.
	\item For any countable collection $A_1, A_2, \dots$ with $A_i \in \FF$ for all $i$, we must also have that $\bigcup_{i \geq 1} A_i \in \FF$.
\end{enumerate}
\end{definition}

\begin{definition}[Probability Measure]
	Suppose that $\FF$ is a $\sigma$-algebra on $\Omega$. Then a function $\PP : \FF \rightarrow [0, 1]$	is called a \vocab{probability measure} if the following are true.
	\begin{enumerate}[label=(\roman*)]
		\item $\PP(\Omega) = 1$.
		\item For any countable disjoint collection $A_1, A_2, \dots$ with $A_i \in \FF$ for all $i$, we have
		$$
		\PP\left(\bigcup_{n \geq 1} A_n\right) = \sum_{n \geq 1} \PP (A_n).
		$$
	\end{enumerate}
	We say that $\PP(A)$ is the \vocab{probability} of $A$.
\end{definition}

\begin{definition}[Probability Space]
	If $\Omega$ is a $\sigma$-algebra on $\Omega$ and $\PP$ is a probability measure, then $(\Omega, \FF, \PP)$ is a \vocab{probability space}.
\end{definition}

When the set $\Omega$ is countable, we take $\FF$ to be all subsets of $\Omega$.

\begin{definition}[Outcomes and Events]
	The elements of $\Omega$ are called \vocab{outcomes}, and the elements of $\FF$ are called \vocab{events}.
\end{definition}

Note that $\PP$ is defined on $\FF$, so it is defined on the \emph{events}, not the \emph{outcomes}. 

Let's look at some properties of the probability measure (which follow immediately from the definition).

\begin{proposition}[Properties of $\PP$]
	If $\PP$ is a probability measure then
	\begin{itemize}
		\item $\PP(A^c) = 1 - \PP(A)$
		\item $\PP(\emptyset) = 0$
		\item If $A \subset B$, then $\PP(A) \leq \PP(B)$
		\item $\PP(A \cup B) = \PP(A) + \PP(B) - \PP(A \cap B)$
	\end{itemize}
\end{proposition}
\begin{proof}[Proof Sketch] Check definitions.
\end{proof}

\begin{example}[Examples of Probability Spaces]
	Some examples of probability spaces are given below.
	\begin{itemize}
		\item \emph{Rolling a fair die}. Consider rolling a fair die. Then
		$
		\Omega = \{1, 2, 3, 4, 5, 6\}
		$, and $\FF$ is all subsets of $\Omega$.
		Then $\PP(\{w\}) = \frac{1}{6}$ for all $\omega \in \Omega$ and if $A \subseteq \Omega$, then $\PP(A) = \frac{|A|}{6}$.
		\item \emph{Equally likely outcomes}. Let $\Omega$ be a finite set, $\Omega = \{\omega_1, \omega_2, \dots, \omega_n \}$, and $\FF$ be all subsets of $\Omega$. Then define $\PP : \FF \rightarrow [0, 1]$ by $\PP(A) = \frac{|A|}{|\Omega|}$. In classical probability, this models picking a random element of $\Omega$. Note that $\PP(\{\omega_i\}) = \frac{1}{|\Omega|}$ for all $\omega_i \in \Omega$.
		\item \emph{Picking balls from a bag}. Suppose we have $n$ balls with $n$ labels from $\{1, \dots, n\}$ that are all indistinguishable by touch. Picking $k \leq n$ balls at random\footnote{That is, with all outcomes equally likely.} without replacement. Then we take $\Omega = \{A \subseteq \{1, 2, \dots, n\} \mid |A| = k \}$, and $\FF$ be all subsets of $\Omega$. Then $|\Omega| = \frac{n}{k}$, and for $\omega \in \Omega$, $\PP(\{\omega\}) = \frac{1}{|\Omega|}$.
		\item \emph{Deck of cards}. Take a well-shuffled deck of 52 cards. Then let $\Omega$ be the set of all permutations of the cards, and note $|\Omega| = 52!$. Then we have $\PP(\text{top 2 cards are aces}) = \frac{4 \cdot 3 \times 50!}{52!} = \frac{1}{221}$.
		\item \emph{Largest digit}. Consider a string of $n$ random digits from $0, \dots, 9$. Then $\Omega = \{0, 1, \dots, 9\}^n$, and $|\Omega| = 10^n$. Now define $A_k = \{\text{no digit exceeds }l\}$ and $B_k = \{\text{largest digit is }k\}$. Then $\PP(B_k) = \frac{|B_k|}{|\Omega|}$. Notice that $B_k = A_k \backslash A_{k - 1}$, and $|A_k| = (k + 1)^n$, so $|B_k| = (k + 1)^n - k^n$, thus $\PP(B_k) = \frac{(k + 1)^n - k^n}{10^n}$.
		\item \emph{Birthday Problem}. There are $n$ people. What is the probability that at least two of them share the same birthday? We can assume nobody is born on $29/02$, and that each birthday is equally likely. So $\Omega = \{1, \dots, 365\}^n$, and $\FF$ is all subsets of $\Omega$. As we assumed all outcomes are equally likely, we take $\PP(\{\omega\}) = \frac{1}{365^n}$ with $\omega \in \Omega$.
		Letting $A = \{\text{at least 2 people share a birthday}\}$, then $A^c = \{\text{all $n$ birthdays are different}\}$, and since $\PP(A) = 1 - \PP(A^c)$, it suffices to calculate $\PP(A^c)$. Now $\PP(A^c) = \frac{|A^c|}{|\Omega|} = \frac{365 \times 364 \times \cdots \times (365 - n + 1)}{365^n}$, and hence $\PP(A) = 1 - \frac{365 \times 364 \times \cdots \times (365 - n + 1)}{365^n}$. If $n = 23$, then this probability is approximately $0.507$.
\end{itemize}
\end{example}

\section{Combinatorial Analysis}

Suppose we have some finite set $\Omega$, and that $|\Omega| = n$. We want to partition $\Omega$ into $k$ disjoint subsets $\Omega_1, \Omega_2, \dots, \Omega_k$ with $|\Omega_i| = n_i$ and $\sum_{i = 1}^k n_i = n$. How many ways is there to do this?

If $M$ is the number of ways, then
$$
M = \binom{n}{n_1} \binom{n - n_1}{n_2} \cdots \binom{n - (n_1 + \cdots + n_{k - 1})}{n_k} = \frac{n!}{n_1! n_2 !\cdots n_k !}.
$$

\begin{definition}[Multinomial Coefficient]
	We define the \vocab{multinomial coefficient} $\binom{n}{n_1, \dots, n_k}$ to be the number of ways of partitioning a set with $n$ elements into $k$ subsets of size $n_1, n_2, \dots, n_k$. We have
	$$
	\binom{n}{n_1, \dots, n_k} = \frac{n!}{n_1! n_2 !\cdots n_k !}.
	$$
\end{definition}
	
Now let's think about the following question: How many strictly increasing and increasing functions are there between two sets?

If we have $f:\{1, \dots, k\} \rightarrow \{1, \dots, n\}$, we say it's \emph{strictly increasing} if whenever $x < y$, then $f(x) < f(y)$. We say that it's \emph{increasing} if $x < y$ implies $f(x) \leq f(y)$.

Any such function is uniquely determined by its range which is a subset of $\{1, \dots, n\}$ of size $k$. There are $\binom{n}{k}$ such subsets, and hence $\binom{n}{k}$ strictly increasing functions.

We define a bijection from $\{f : \{1, \dots, k \} \rightarrow \{1, \dots, n\} \mid f \text{ increasing}\}$ to $\{g : \{1, \dots, k \} \rightarrow \{1, \dots, n + k - 1\} \mid f \text{ strictly increasing}\}$. For each $f$, we define $g(i) = g(i) + i - 1$. Then $g$ is strictly increasing and takes values in $\{1, \dots, n + k - 1\}$. Then $g$ is strictly increasing and takes values in $\{1, \dots, n + k - 1\}$.

So the total number of increasing functions $f: \{1, \dots, k\} \rightarrow \{1, \dots, n\}$ is $\binom{n + k - 1}{k}$.

\section{Stirling's Formula}

Frequently in probability it will help to have some bounds/an asymptotic expression for the factorial.

\begin{notation}
Let $(a_n)$ and $(b_n)$ be two sequences. We will write $a_n \sim b_n$ if $\frac{a_n}{b_n} \rightarrow 1$ as $n \rightarrow \infty$.
\end{notation}

We will first prove a weak approximation of the factorial.

\begin{proposition}
	$\log(n!) \sim n \log n$ as $n \rightarrow \infty$.
\end{proposition}
\begin{proof}
	For $x \in \R$, we write $\lfloor x \rfloor$ for the \emph{integer part} of $x$. Then we have $\log\lfloor x \rfloor \leq \log x \leq \log \lfloor x + 1 \rfloor$. Integrating this from $1$ to $n$, we get
	\begin{align*}
		\sum_{k = 1}^{n - 1} \log k &\leq \int_1^n \log x \; \mathrm{d} x \leq \sum_{k = 1}^n \log k\\
\implies \log (n - 1)! &\leq n \log n - n + 1 \leq \log n!
	\end{align*}
	Using this, we get that
	$$
	n \log n - n + 1 \leq \log n! \leq (n + 1) \log (n + 1) - (n + 1) + 1,
	$$
	and dividing through by $n \log n$ we get
	$$
	\frac{\log n!}{n \log n} \rightarrow 1 \quad \quad n \rightarrow \infty.
	$$
\end{proof}

Now let's prove Stirling's formula. Note that the proof is non-examinable.


\begin{theorem}[Stirling Approximation]
	$n! \sim n^n \sqrt{2 \pi n} \; e^{-n}$ as $n \rightarrow \infty$.
\end{theorem}
\begin{proof}[Proof (Non-Examinable).]
	For any function $f$ that is twice differentiable, and $a < b$, then
	$$
	\int_a^b f(x) \; \mathrm{d}x = \frac{f(a) + f(b)}{2} (b - a) - \frac{1}{2} \int_a^b (x - a)(b - x)f''(x) \; \mathrm{d}x.
	$$
	This follows by integrating by parts.

	Now take $f(x) = \log x$, and $a = k$, $b = k + 1$. Then substituting into the formula, we get
	\begin{align*}
		\int_k^{k + 1} \log x \; \mathrm{d}x &= \frac{\log k + \log(k + 1)}{2} - \frac{1}{2} \int_k^{k + 1} \frac{(x - k)(k + 1 - x)}{x^2} \; \mathrm{d}x 	 \\
		&= \frac{\log k + \log (k + 1)}{2} + \frac{1}{2} \int_0^1 \frac{x(1 - x)}{(x + k)^2} \; \mathrm{d}x.
	\end{align*}
	Then taking the sum for $k = 1, \dots, n - 1$ of the equality above, we get
	\begin{align*}
		\int_1^n \log x \; \mathrm{d}x &= \frac{\log(n - 1)! + \log n!}{2} + \frac{1}{2} \sum_{k = 1}^{n - 1} \int_0^1 \frac{x (1 - x)}{(x + k)^2} \; \mathrm{d}x \\
\implies \log n-n+1 &= \log (n !)-\frac{\log n}{2}+\sum_{k=1}^{n-1} a_{k},
	\end{align*}
	where we set
	$$
	a_{k}=\frac{1}{2} \int_{0}^{1} \frac{x(1-x)}{(x+k)^{2}} \; \mathrm{d} x.
	$$
	But then
	\begin{align*}
		\log n ! &=n \log n-n+\frac{\log n}{2}+1-\sum_{k=1}^{n-1} a_{k} \\
\implies n !&=n^{n} e^{-n} \cdot \sqrt{n} \exp \left(1-\sum_{k=1}^{n-1} a_{k}\right) .
	\end{align*}
	Note that $a_{k} \leqslant \frac{1}{2} \int_{0}^{1} \frac{x(1-x)}{k^{2}} d x=\frac{1}{12 k^{2}}$, so $\sum a_k < \infty$. We set $A=\exp \left(1-\sum_{k=1}^{\infty} a_{k}\right) $. Then
	$$
	n !=n^{n} \cdot e^{-n} \sqrt{n} \cdot A \cdot \exp \left(\sum_{k=n}^{\infty} a_{k}\right),
	$$
	and as $\exp \left(\sum_{k=n}^{\infty} a_{k}\right) \rightarrow 1$ (since the argument goes to 0), we have proved that
	$$
	\frac{n!}{n^n e^{-n} \sqrt{n}} \rightarrow A, \quad \quad \text{as }n \rightarrow \infty,
	$$
	which means that $n! \sim n^n e^{-n} \sqrt{n} \cdot A$ as $n \rightarrow \infty$.

	To finish the proof, we need to show that $A = \sqrt{2 \pi}$.
	Knowing that $n! \sim n^n e^{-n} \sqrt{n} \cdot A$ as $n \rightarrow \infty$, we have
	$$
2^{-2n} \cdot \binom{2n}{n} = 2^{-2n} \frac{(2n)!}{n!\cdot n!} \sim \frac{2^{-2n} \cdot (2n)^{2n} \cdot \sqrt{2n} \cdot A \cdot e^{-2n}}{n^n \cdot e^{-n}\cdot \sqrt{n} \cdot A \cdot n^n \cdot e^{-n} \cdot \sqrt{n} \cdot A} = \frac{\sqrt{2}}{A \sqrt{n}}.
	$$
	Using a different method we will prove that
	$$
	2^{2n} \binom{2n}{n} \sim \frac{1}{\sqrt{\pi n}},
	$$
	which will force $A = \sqrt{2 \pi}$.

	Consider the integral
	$$
	I_n = \int_0^{2 \pi} (\cos \theta)^n \; \mathrm{d}\theta, \quad \quad n \geq 0.
	$$
	So $I_0 = \pi/2$ and $I_1 = 1$. Then integrating by parts, we get $I_n = \frac{n - 1}{n} T_{n - 2}$, and thus
	$$
	I_{2n} = \frac{2n - 1}{2n} \cdot I_{2n - 2} = \frac{(2n - 1)(2n - 3)\cdots 3 \cdot 1}{2n \cdot (2n - 2) \cdots 2} I_0 = \frac{(2n)!}{2^{2n} n! \cdot n!} \cdot \frac{\pi}{2},
	$$
	so
	$$
	I_{2n} = 2^{-2n} \binom{2n}{n} \frac{\pi}{2}.
	$$
	In the same way we get
	$$
	I_{2n + 1} = \frac{2n \cdots 4 \cdot 2}{(2n + 1) \cdots 3 \cdot 1} I_1 = \frac{1}{2n + 1} \left(2^{-2n} \binom{2n}{n}\right)^{-1}.
	$$
	From $I_n = \frac{n - 1}{n}I_{n - 2}$ we get $\frac{I_n}{I_{n - 2}}\rightarrow 1$ as $n \rightarrow \infty$, and what we want is
	$\frac{I_{2n}}{I_{2n + 1}} \rightarrow 1$ as $n \rightarrow \infty$.

	Note that $I_n$ is a decreasing function of $n$, therefore
	$$
	\frac{I_{2n}}{I_{2n + 1}} \leq \frac{I_{2n - 1}}{I_{2n + 1}} \rightarrow 1,
	$$
	and also
	$$
	\frac{I_{2n}}{I_{2n + 1}} \geq \frac{I_{2n}}{I_{2n - 2}} \rightarrow 1,
	$$
	thus $\frac{I_{2n}}{I_{2n+ 1}} \rightarrow 1$ as $n \rightarrow \infty$, which means
	\begin{align*}
		\frac{2^{-2n} \binom{2n}{n} \frac{\pi}{2}}{\left(2^{-2n} \binom{2n}{n}\right)^{-1} \frac{1}{2n + 1}} &\rightarrow 1 \\
\implies \left(2^{-2n} \binom{2n}{n}\right)^{2} \frac{\pi}{2} (2n + 1) \rightarrow 1,
	\end{align*}
	thus
	$$
	\left(2^{-2n} \binom{2n}{n}\right)^{2} \sim \frac{2}{\pi (2n + 1)} \sim \frac{1}{\pi n},
	$$
	thus $A = \sqrt{2 \pi}$, which completes the proof.
\end{proof}

\chapter{Properties of Probability Measures}

Let $(\Omega, \FF, \PP)$ be a probability space. Recall that the probability measure $\PP$ is a function
$$
\PP : \FF \rightarrow [0, 1]
$$
with $\PP(\Omega) = 1$ which is \emph{countable additive}, that is, for any countable disjoint collection $A_1$, $A_2$, $\dots$ with $A_i \in \FF$ for all $i$,
$$
\mathbb{P}\left(\bigcup_{n \geq 1} A_{n}\right)=\sum_{n \geq 1} \mathbb{P}\left(A_{n}\right).
$$

\section{Countable Subadditivity}

When the sequence is not necessarily disjoint, this equality becomes an inequality. 

\begin{proposition}[Countable Subadditivity]
	Let $(A_n)$ be a sequence of events with $A_i \in \FF$ for all $i$. Then we have
	$$
	\mathbb{P}\left(\bigcup_{n \geq 1} A_{n}\right) \leq \sum_{n \geq 1} \mathbb{P}\left(A_{n}\right).
	$$
\end{proposition}
\begin{proof}
	Define $B_1 = A_1$ and $B_n = A_n \backslash (A_1 \cup \cdots \cup A_{n - 1})$ for all $n \geq 2$. Then $(B_n)$ is a disjoint sequence of events in $\FF$, and $\bigcup_{n \geq 1} B_n = \bigcup_{n \geq 1} A_n$. So $\PP(\bigcup A_n) = \PP(\bigcup B_n)$. By countable additivity for $(B_n)$,
	$$
	\PP \left(\bigcup_{n \geq 1} B_n\right) = \sum_{n \geq 1} \PP(B_n).
	$$
	But $B_n \subseteq A_n$, so $\PP(B_n) \leq \PP(A_n)$ for all $n$. Therefore
	$$
	\PP(\bigcup A_n) = \PP(\bigcup B_n) = \sum \PP(B_n) \leq \sum_{n \geq 1} \PP(A_n).
	$$
\end{proof}

\section{Continuity of Probability Measures}

We have continuity for probability measures as follows.

\begin{proposition}[Continuity of Probability Measures]
	Let $(A_n)$ be an increasing sequence in $\FF$, 
	so that $A_1 \subseteq A_2 \subseteq \cdots$. 
	We know that $\PP(A_n) \leq \PP(A_{n + 1})$. 
	So $\PP(A_n)$ converges as $n \rightarrow \infty$, and $\lim_{n \to \infty} \PP(A_n) = \PP\left(\bigcup_n A_n\right)$.
\end{proposition}
\begin{proof}
	Set $B_1 = A_1$ and for $n \geq 2$ $B_n = A_n \backslash (A_1 \cup \cdots \cup A_{n - 1})$. Then
	$$
	\bigcup_{k =1}^{n} B_k = A_n, \quad \text{and} \quad \bigcup_{k = 1}^\infty B_k = \bigcup_{k = 1}^{\infty} A_k.
	$$
	So $\PP(A_n) = \PP\left(\bigcup_{k = 1}^n B_k\right) = \sum_{k = 1}^n \PP(B_k) \rightarrow \sum_{k = 1}^{\infty} \PP(B_k)$ as $n \rightarrow \infty$.

	It remains to prove that $\sum_{k = 1}^{\infty} \PP(B_k) = \PP(\bigcup A_n)$. Since $\bigcup_{k = 1}^{\infty} B_k = \bigcup_{k = 1}^{\infty} A_k$, we get $\PP(\bigcup A_n) = \PP(\bigcup B_n) = \sum_n \PP(B_n)$.
\end{proof}

Similarly, if $(A_n)$ is a decreasing sequence in $\FF$, that is, $A_1 \supseteq A_2 \supseteq \cdots$, then $\PP(A_n) \rightarrow \PP(\cap_n A_n)$ as $n \rightarrow \infty$.

\section{Inclusion-Exclusion Formula}

Suppose that $A, B \in \FF$. Then $\PP(A \cup B) = \PP(A) + \PP(B) - \PP(A \cap B)$. If we also have $C \in \FF$, then repeatedly applying the previous we have
\begin{align*}
	\PP(A \cup B \cup C) = \PP(A) + \PP(B) + \PP(C) - \PP(A \cap B) - \PP(A \cap C) - \PP(B \cap C) + \PP(A \cap B \cap C).
\end{align*}
In general, we have the \emph{inclusion-exclusion formula}.

\begin{proposition}[Inclusion-Exclusion Formula]
	Let $A_1, \dots, A_n \in F$. Then
	$$
	\PP\left(\bigcup_{i = 1}^n A_i\right) 
	= \sum_{k = 1}^{n} (-1)^{k + 1} \left(\sum_{1 \leq i_1 < \cdots < i_k \leq n} \PP(A_{i_1} \cap A_{i_2} \cap \cdots A_{i_k})\right).
	$$
\end{proposition}
\begin{proof}
	We will use induction. For $n = 2$ it holds by definition. Now assume it holds for $n - 1$ events. We have
	$$
	\PP((A_1 \cup \cdots \cup A_{n-1}) \cup A_n) = \PP(A_1 \cup \cdots \cup A_{n-1}) + \PP(A_n) - \PP((A_1 \cup \cdots \cup A_{n-1}) \cap A_n),
	$$
	and we can rewrite the intersection term as $\PP((A_1 \cap A_n) \cup \cdots \cup (A_{n - 1} \cap A_n))$. Setting $B_i = A_i \cap A_n$, then by the inductive hypothesis we have
	$$
	\PP(A_1 \cup \cdots A_{n - 1}) = \sum_{k = 1}^{n-1} (-1)^{k + 1} \left(\sum_{1 \leq i_1 < \cdots < i_k \leq n-1} \PP(A_{i_1} \cap A_{i_2} \cap \cdots A_{i_k})\right),
	$$
	and
	$$
	\PP(B_1 \cup \cdots B_{n - 1}) = \sum_{k = 1}^{n-1} (-1)^{k + 1} \left(\sum_{1 \leq i_1 < \cdots < i_k \leq n-1} \PP(B_{i_1} \cap B_{i_2} \cap \cdots B_{i_k})\right),
	$$
	Plugging these into our expression gives us the required claim.
\end{proof}

Let $(\Omega, \FF, \PP)$ with $\Omega$ finite be a probability space, with $\PP(A) = \frac{|A|}{|\Omega|}$ for all $A \in F$. Let $A_1, \dots, A_n \in \FF$. Then
$$
|A_1 \cup \cdots \cup A_n| = \sum_{k = 1}^n (-1)^{k + 1} \sum_{1 \leq i_1 < \cdots < i_k \leq n} |A_{i_1} \cap \cdots \cap A_{i_k}|.
$$

We can also think about what happens if we truncate the inclusion exclusion formula at some point. For example, for two events we have
$$
\PP(A \cup B) = \PP(A) + \PP(B) - \PP(A \cap B) \implies \PP(A\cup B) \leq \PP(A) + \PP(B),
$$
which is an upper bound and for three events we have
$$
\PP(A \cup B \cup C) \geq \PP(A) + \PP(B) +  \PP(B) - \PP(A \cap B) - \PP(A \cap C) - \PP(B \cap C),
$$
which is a lower bound. We can generalize this as follows.

\begin{proposition}[Bonferroni Inequalities]
	Truncating the sum in the inclusion-exclusion formula at the $r$th term gives an overestimate if $r$ is odd, and an underestimate if $r$ is even.
\end{proposition}
\begin{proof}
	We will again use induction. For $n = 2$ we know $\PP(A \cup B) \leq \PP(A) + \PP(B)$. Now assume the claim holds for $n - 1$ events. Suppose that $r$ is odd. Then $\PP(A_1 \cup \dots A_n) = \PP(A_1 \cup \dots A_{n-1}) + \PP(A_n) - \PP(B_1 \cap \cdots \cap B_{n - 1})$, where $B_i = A_i \cap B_n$.

	Sine $r$ is odd, apply the inductive hypothesis to $\PP(A_1 \cup \cdots A_{n-1})$ to get
	$$
	\PP(A_1 \cup \cdots A_{n - 1}) \leq \sum_{k = 1}^{r} (-1)^{k + 1} \left(\sum_{1 \leq i_1 < \cdots < i_k \leq n-1} \PP(A_{i_1} \cap A_{i_2} \cap \cdots A_{i_k})\right),
	$$
	and since $r - 1$ is even, we can apply the inductive hypothesis to $\PP(B_1 \cup \cdots B_{n - 1})$ to get
	$$
	\PP(B_1 \cup \cdots B_{n - 1}) \geq \sum_{k = 1}^{r - 1} (-1)^{k + 1} \left(\sum_{1 \leq i_1 < \cdots < i_k \leq n-1} \PP(B_{i_1} \cap B_{i_2} \cap \cdots B_{i_k})\right).
	$$
	Substituting both upper bounds in the original expression, we get an overestimate. The case for $r$ even follows analogously.
\end{proof}

We can use the inclusion-exclusion to count combinatorially.

\begin{example}[Number of Surjective Functions]
	We will find the number of surjections $f: \{1, \dots, n\} \rightarrow \{1, \dots, m\}$.

	Let $\Omega$ be the set of functions from $\{1, \dots, n\} \rightarrow \{1, \dots, m\}$, and $A$ be the subset of surjective functions in $\Omega$. We wish to find $|A|$.

	For all $i \in \{1, \dots, m\}$, we define $A_i = \{f \in \Omega \mid i \not\in \{f(1), \dots, f(n)\}\}$. Then $A = A_1^c \cap A_2^c \cap \cdots \cap A_m^c = (A_1 \cup \cdots A_m)^c$. Thus $|A| = |\Omega| - |A_1 \cup \cdots \cup A_m| = m^n - |A_1 \cup \cdots \cup A_m|$. Now we have (by inclusion-exclusion)
	$$
	|A_1 \cup \cdots \cup A_m| =\sum_{k = 1}^n (-1)^{k + 1} \sum_{1 \leq i_1 < \cdots < i_k \leq n} |A_{i_1} \cap \cdots \cap A_{i_k}|.
	$$
	We can count $|A_{i_1} \cap \cdots \cap A_{i_k}| = (m - k)^n$.
	Thus
	$$
	|A_1 \cup \cdots \cup A_m| = \sum_{k=1}^m (-1)^{k + 1} \binom{m}{k} (m - k)^n
	$$
	So $|A| = \sum_{k = 0}^m (-1)^k \binom{m}{k}(m - k)^n$.
\end{example}


\subsection*{Counting Derangements}

A derangement is a permutation that has no fixed points.

Let $\Omega$ be the set of permutations of $\{1, 2, \dots, n\}$, and $A$ be the set of derangements, $A = \{f \in \Omega \mid f(i) \neq i \text{ for } i = 1,2, \dots, n\}$.
We pick a permutation at random, and we want to know the probability that it is in $A$.

Define $A_i = \{ f \in \Omega \mid f(i) = i\}$. Then $A = A_1^c \cap \cdots \cap A_n^c = \left(\bigcup_{i = 1}^{n} A_i\right)^c$. So $\PP(A) = 1 - \PP(\bigcup_{i = 1}^n A_i)$. By inclusion exclusion,
\begin{align*}
\mathbb{P}\left(\bigcup_{i=1}^{n} A_{i}\right)&=\sum_{k=1}^{n}(-1)^{k+1} \sum_{1 \leq i_{1}<\ldots<i_{k} \leq n} \mathbb{P}\left(A_{i_1} \cap \ldots \cap A_{i_{k}}\right)\\
&= \sum_{k = 1}^n (-1)^{k + 1} \binom{n}{k} \cdot \frac{(n - k)!}{n!} \\
&= \sum_{k = 1}^n (-1)^{k + 1} \frac{n!}{k! \cdot (n - k)!}
 \cdot \frac{(n - k)!}{n!} \\
 &= \sum_{k = 1}^n \frac{(-1)^{k + 1}}{k!}.
 \end{align*}
Thus $\PP(A) = 1 - \sum_{k = 1}^n \frac{(-1)^{k + 1}}{k!} = \sum_{k = 0}^{n} \frac{(-1)^k}{k!}$, and as $n \rightarrow \infty$, we have $\PP(A) \rightarrow e^{-1} \approx 0.3678$.

\chapter{Independence and Conditional Probability}

We will now look at cases where events are `independent', and also how we can work with probabilities, given that we know some condition is true.

\section{Independence}

If we have some probability space $(\Omega, \FF, \PP)$, we have the notion of \emph{independence}.

\begin{definition}[Independence]
Let $A, B \in \FF$. They are called \vocab{independent} if $\PP(A \cap B) = \PP(A) \cdot \PP(B)$.

A countable collection of events $(A_n)$ is said to be \vocab{independent} if for all distinct $i_1, i_2, \dots, i_k$, we have
$$
\PP(A_{i_1} \cap \cdots \cap A_{i_k}) = \prod_{j = 1}^k \PP(A_{i_j}).
$$
\end{definition}

Note that pairwise independence does not imply independence.

\begin{example}[Pairwise Independence is not Independence]
	If we toss a fair coin twice, we have $\Sigma = \{(0, 0), (0, 1), (1, 0), (1, 1)\}$, and $\PP(\{\omega\}) = 1/4$ for all $\omega \in \Omega$.

Define $A = \{(0, 0), (0, 1)\}$, $B = \{(0, 0), (1,0)\}$ and $C = \{(1, 0), (0, 1)\}$. Then $\PP(A) = \PP(B) = \PP(C) = 1/2$. Also $\PP(A \cap B) = \PP(\{(0, 0)\}) = 1/4 = 1/2 \cdot 1/2 = \PP(A) \cdot \PP(B)$. Thus $A$ and $B$ are independent. Similarly, $B$ and $C$ are independent, and $A$ and $C$ are independent.

However, $\PP(A \cap B \cap C) = \PP(\emptyset) = 0 \neq \PP(A) \cdot \PP(B) \cdot \PP(C)$, so $A$, $B$ and $C$ are not independent. 
\end{example}

\begin{proposition}
	If $A$ is independent of $B$, then $A$ is also independent of $B^c$.
\end{proposition}
\begin{proof}
	$\PP(A \cap B^c) = \PP(A) - \PP(A \cap B) = \PP(A) - \PP(A) \cdot \PP(B)$, by the independence of $A$ and $B$. Then this is $\PP(A)\cdot (1 - \PP(B)) = \PP(A) \cdot \PP(B^c)$.
\end{proof}


\section{Conditional Probability}

We can now think of this idea of probability based on conditions.

\begin{definition}[Conditional Probability]
	Suppose we had some event $B \in \FF$ with $\PP(B) > 0$, and let $A \in \FF$. We define the \vocab{conditional probability} of $A$ given $B$
and write $\PP(A \mid B)$ to be
$$
\PP(A \mid B) = \frac{\PP(A \cap B)}{\PP(B)}.
$$
\end{definition}

If $A$ and $B$ are independent, then $\frac{\PP(A \cap B)}{\PP(B)} = \frac{\PP(A) \cdot \PP(B)}{\PP(B)} = \PP(A)$. So in this case, $\PP(A \mid B) = \PP(A)$.

We can also generalise this slightly.

\begin{proposition}
	Suppose $(A_n)$ is a disjoint sequence in $\FF$. Then
$$
\PP\left(\bigcup A_n \mid B\right) = \sum_n \PP(A_n \mid B).
$$
\end{proposition}
\begin{proof}
	By countable additivity, we have
	\begin{align*}
		\PP(\bigcup A_n \mid B) &= \frac{\PP((\bigcup A_n) \cap B)}{\PP(B)} \\
		&= \sum_n \frac{\PP(A_n \cap B}{\PP(B)}\\
		&= \sum_n \PP(A_n \mid B).
	\end{align*}
\end{proof}

We will also use the following result frequently.

\begin{proposition}[Law of Total Probability]
	Suppose $(B_n)$ is a disjoint collection in $\FF$, and $\bigcup B_n = \Omega$, and $\PP(B_n) > 0$ for all $n$. Let $A \in \FF$. Then 
	$$
	\PP(A) = \sum_n \PP(A \mid B_n) \PP(B_n).
	$$
\end{proposition}
\begin{proof}
	$\PP(A) = \PP(A \cap \Omega) = \PP(A \cap (\bigcup_n B_n))$, and by countable additivity of $\PP$, $\sum_{n}\PP(A \cap B_n) = \sum_n \PP(A\mid B_n) \cdot \PP(B_n)$.
\end{proof}

\begin{proposition}[Bayes' Formula]
	Let $(B_n)$ be a disjoint collection of events, with $\cup B_n = \Omega$ and $\PP(B_n) > 0$ for all $n$. Then
	$$
	\PP(B_n \mid A) = \frac{\PP(A \mid B_n) \cdot \PP(B_n)}{\sum_k \PP(A\mid B_k) \cdot\PP(B_k)}. 
	$$
\end{proposition}
\begin{proof}
	We have
	$$
	\PP(B_n \mid A) = \frac{\PP(B_n \cap A)}{\PP(A)} = \frac{\PP(A\mid B_n) \cdot \PP(B_n)}{\PP(A)},
	$$
	and by the law of total probability, $\PP(A) = \sum_k \PP(A \mid B_k) \cdot \PP(B_k)$.
\end{proof}

This formula is the basis of Bayesian statistics.

We know the probabilities of the events $(B_k)$, and we have a model which gives us the conditional probabilities $\PP(A \mid B_n)$. Bayes' formula tells us how to calculate the posterior probabilities of $B_n$ given that the event $A$ occurs.

\begin{example}[False Positive of a Rare Diease]
	Suppose that some rare disease $A$ affects $0.1\%$ of the population. We have a medical test that is positive for $98\%$ of the affected population and $1\%$ of those unaffected by the disease. Suppose we picked an individual at random. What is the probability that they suffer from the disease $A$ given that they tested positive?

	We define $A = \{\text{individual suffers from $A$}\}$ and $P =\{\text{individual tested positive}\}$. We want $\PP(A \mid P)$.

	We have $\PP(A) = 0.001$, and $\PP(P \mid A) = 0.98$, and $\PP(A \mid A^c) = 0.01$. Then
	\begin{align*}
		\PP(A \mid P) &= \frac{\PP(P \mid A) \cdot \PP(A)}{\PP(P \mid A) \cdot \PP(A) + \PP(P \mid A^c) \cdot \PP(A^c)} \\
		&= \frac{0.98 \cdot 0.001}{0.98 \cdot 0.001 + 0.01\cdot 0\cdot 999} = 0.089\dots \approx 0.09.
	\end{align*}
	So $\PP(A \mid P) = 0.09$. 

	The reason why this is so low is that $\PP(A \mid A^c)$ is much larger than $\PP(A)$.
\end{example}

\begin{example}[Extra Knowledge Gives Surprising Results]
	Consider the following three statements:
	\begin{enumerate}[label=(\alph*)]
		\item I have two children, one of which is a boy.
		\item I have two children, and the eldest one is a boy.
		\item I have two children, one of whom is a boy born on a Thursday.
	\end{enumerate}
	In each case, we want to know $\PP(\text{I have 2 boys} \mid a)$ (or b or c).

	Since no further information is given, we take all outcomes to be equally likely.

	Define the event $BG$ where the eldest is a boy, youngest is a girl. Also define the event $GB$ where the eldest is a girl and youngest is a boy. Lastly define events $BB, GG$ for two boys or two girls respectively.
	
	Now consider the various statements
	\begin{enumerate}[label=(\alph*)]
		\item $\PP(BB \mid BB \cup BG \cup GB) = \frac{1}{3}$.
		\item $\PP(BB \mid BB \cup BG) = \frac{1}{2}$.
		\item Define the event $GT$ where the eldest is a girl and the youngest is a boy born on a Thursday. Also define $TN$ where the eldest is a boy born on a Thursday, and the youngest is a boy not born on a Thursday. Similarity define $TT$, $TG$, and $NT$.
		
		Then $\PP((TT\cup TN \cup NT) \mid  (GT \cup TG \cup TT \cup TN \cup NT)) = \frac{13}{27}$.
	\end{enumerate}
\end{example}

\begin{example}[Simpson's Paradox]
	Consider a program that has 100 applicants, 50 of which are women and 50 of which are men. The table below shows the probability of applicants getting into the program based on what type of school they went to.
	\begin{center}
	\begin{tabular}{|c|c|c|c|}
		\hline All applicants & Admitted & Rejected & $\%$ Admitted \\
		\hline State & 25 & 25 & $50 \%$ \\
		\hline Independent & 28 & 22 & $56 \%$ \\
		\hline
		\end{tabular}
	\end{center}
	Now the next two tables show this information for men only and women only.
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline Men only & Admitted & Rejected & \% Admitted \\
			\hline State & 15 & 22 & $41 \%$ \\
			\hline Independent & 5 & 8 & $38 \%$ \\
			\hline
			\end{tabular}
		\end{center}
		\begin{center}
			\begin{tabular}{|c|c|c|c|}
				\hline Women only & Admitted & Rejected & $\%$ Admitted \\
				\hline State & 10 & 3 & $77 \%$ \\
				\hline Independent & 23 & 14 & $62 \%$ \\
				\hline
				\end{tabular}
	\end{center}

	Note that in both the men only and women only, the percentage admitted from independent schools was \emph{lower} than from state schools, but in the total applicants the percentage admitted from independent schools was \emph{higher}.

	This phenomenon is called \emph{confounding} in statistics, and arises when we aggregate data from disparate populations.

	Define $A$ to be the event that an individual is admitted, $B$ that they are a man, $B^c$ that they are a woman, $C$ that they come from a state school, and $C^c$ that they come from an independent school. Then we see that
	$$
	\PP(A \mid B \cap C) > \PP(A \mid B \cap C^c), \quad \text{and} \quad \PP(A \mid B^c \cap C) > \PP(A \mid B^c \cap C^c),
	$$
	but in the example above we have $\PP(A \mid C^c) > \PP(A \mid C)$.

	So
	\begin{align*}
		PP(A \mid C) &= \PP(A \cap B \mid C) + \PP(A \cap B^c \mid C) \\
		&= \frac{\PP(A \cap B \cap C)}{\PP(C)} + \frac{\PP(A \cap B^c \cap C)}{\PP(C)} \\
		&= \PP(A \mid B \cap C) + \PP(A \mid B^c \cap C) \cdot \PP(B^c \mid C) \\
		&> \PP(A \mid B \cap C^c) \cdot \PP(B \mid C) + \PP(A \mid B^c \cap C^c) \PP(B^c \mid C).
	\end{align*}
	Assuming that $\PP(B \mid C) = \PP(B \mid C^c)$, though this wasn't the case in the example, then
	\begin{align*}
		\PP(A \mid C) &> \PP(A \mid B \cap C^c) \cdot \PP(B \mid C^c) + \PP(A \mid B^c \cap C^c) \cdot \PP(B^c \mid C^c) \\
		&= \PP(A \mid C^c).
	\end{align*}
	So under this extra assumption, we would get that indeed $\PP(A \mid C) > \PP(A \mid C^c)$.
\end{example}

\chapter{Discrete Probability Distribution}

In this chapter we will discuss probability distributions that arise
with a discrete random variable. 

Consider a probability space $(\Omega, \FF, \PP)$, where $\Omega$ is finite or countable, so $\Omega = \{\omega_1, \omega_2, \dots \}$ and $\FF$ is all subsets of $\Omega$.

If we know $\PP(\{\omega_i\})$ for all $i$, then this determines $\PP$. Indeed, let $A \subseteq \Omega$. Then $\PP(A) = \PP(\bigcup_{\omega_i \in A } \{\omega_i \}) = \sum_{\omega_i \in A} \PP(\{\omega_i\})$, by countable subadditivity.
From this we get the discrete probability distribution.

\begin{definition}[Discrete Probability Distribution]
	Let $(\Omega, \FF, \PP)$ be a probability space, where $\Omega = \{\omega_1, \omega_2, \dots \}$. 
	We write $p_i \PP(\{\omega_i\})$, and we call it a \vocab{discrete probability distribution}.
\end{definition}

Notable, we have $p_i \geq 0$ for all $i$, and $\sum_i p_i = 1$


\section{Common Discrete Probability Distributions}

In the following subsections, we will look at some common discrete probability distributions that arise regularly.

\subsection{Bernoulli Distribution}

The Bernoulli distribution models `the outcome of a coin toss'.
Here, $\Omega = \{0, 1\}$, and $p_1 = \PP(\{1\}) = p$, and $p_0 = \PP(\{0\}) = 1 - p$.

\subsection{Binomial Distribution}

The binomial distribution parameters, $B(N, p)$ where $n \in \Z^+$ and $p \in [0, 1]$. It models the toss of a $p$-coins (where the probability of heads is $p$) $N$ times independently.

We get
$$
	\PP(\text{we see $k$ heads}) = \binom{N}{k}p^k (1 - p)^{N - k}.
$$

So in this distribution, $\Omega = \{0, \dots, N\}$, and $p_i = \binom{N}{i}p^i(1 - p)^{N - i}$.

\subsection{Multinomial Distribution}

This distribution is $M(N, p_1, \dots, p_k)$ with $N \in \Z^+$, $p_1, \dots, p_k \geq 0$ and $\sum_i p_i = 1$.

It models having $k$ boxes and $N$ balls, where we throw the balls into random boxes with probabilities as given. 

We have $\sigma = \{(n_1, \dots, n_k) \in N^k \mid \sum_{i = 1}^{k} = N\}$, and
$$
\PP(\text{$n_1$ balls in box $1$, \dots, $n_k$ balls in box $k$}) = \binom{N}{n_1, \dots, n_k} p_1^{n_1} \cdots p_k^{n_k}.
$$

\subsection{Geometric Distribution}

This distribution corresponds to tossing a $p$-count until the first head appears. 

Here $\Omega = \{1, 2, \dots \}$, and $p_k$ is $\PP(\text{tossed $K$ times until the first H}) = (1 - p)^{k - 1} \cdot p$. Also $\sum_{k = 1}^{\infty} p_k = 1$.

We can also define it as $\Sigma = \{0, 1, \dots\}$ with $\PP(\text{$k$ tails before the first $H$}) = (1 - p)^k \cdot p$.

\subsection{Poisson Distribution}

This distribution is used to model the number of occurrences of an event in a given interval of time. For instance, the number of customers that enter a shop in a day.

We have $\Omega = \{0, 1, 2, \dots \}$, and a parameter $\lambda > 0$ with $\lambda \in \R$. We define $p_k = e^{- \lambda} \cdot \frac{\lambda^k}{k!}$ for all $k \in \omega$. We call this the Poisson distribution with parameter $\lambda$. 

We note $\sum_{k =0}^{\infty} = e^{- \lambda} \sum_{k = 1}^{\infty} \frac{\lambda^k}{k!} = e^{-\lambda} \cdot e^{\lambda} = 1$, so this is indeed a probability distribution.

Suppose that customers arrive into a shop during $[0, 1]$. 
If we discretise $[0, 1]$, that is, subdivide it into $N$ intervals $\left[\frac{i - 1}{N}, \frac{i}{N}\right]$, $i = 1, \dots, N$.
In each interval, a customer arrives with probability $p$, independent of other intervals, and nobody arrives with probability $1 - p$.
So $\PP(\text{$k$ customers arrived}) = \binom{N}{k} \cdot p^k(1 - p)^{N - k}$.

Take $p = \lambda/N$. Then
$$
\binom{N}{k} \cdot p^{k} \cdot (1 - p)^{N - k} = \frac{N!}{k! (N - k)!} \left(\frac{\lambda}{N}\right)^k \cdot \left(1 - \frac{\lambda}{N}\right)^{N - k} = \frac{\lambda^k}{k!} \frac{N!}{N^k(N - k)!} \left(1 - \frac{\lambda}{N}\right)^{N - k}.
$$
Keeping $k$ fixed and letting $N \rightarrow \infty$, this gives us
$$
	\PP(\text{$k$ customers arrived}) \rightarrow e^{- \lambda} \cdot \frac{\lambda^k}{k!}, \quad \text{as } N \rightarrow \infty.
$$
Thus $B(N, p)$ with $o = \lambda/N$ converges to the Poisson distribution with parameter $\lambda$.

\chapter{Random variables}

\section{Definitions}

Given some probability space $(\Omega, \FF, \PP)$, a random variable $X$ is a function $X: \Omega \rightarrow \R$ satisfying $\{\omega \in \Omega \mid X(\omega)\leq< x \} \in \FF$, for all $x \in \R$.

We will use the shorthand notation: suppose $A \subseteq \R$. Then $\{X \in A \} = \{\omega \mid X(\omega) \in A\}$.

Given $A \in \FF$, define the indicator of $A$ to be
$$
1_A(\omega) = \begin{cases}
	1 &\mbox{if } \omega \in A, \\
	0 &\mbox{otherwise }
   \end{cases}
$$
Because $A \in \FF$, $1_A$ is a random variable.

Suppose $X$ is a random variable. We define the probability distribution function of $X$ to be $F_X(x) = \PP(X \leq x)$. Note that $F_X: \R \rightarrow [0, 1]$.

\begin{definition}[Random Variable in $\R^n$]
	$(X_1, \dots, X_n)$ is a \vocab{random variable} in $\R^n$ if $(X_1, \dots, X_n) : \Omega \rightarrow \R^n$ and for all $x_1, \dots, x_n \in R$, we have $\{X_1 \leq x_1, \dots, X_n \leq x_n\} \in \F$.
\end{definition}

Note that this definition is equivalent to saying that $X_1, \dots, X_n$ are all random variables in $\R$. 

\section{Discrete Random Variables}

We are going to look at the specific case of a discrete random variable.

\begin{definition}[Discrete Random Variable]
	A random variable $X$ is called \vocab{discrete} if it takes values in a countable set.
\end{definition}

Suppose $X$ takes values in the countable set $S$. Then for every $x \in S$, we write $p_x = \PP(X = x) = \PP(\{ \omega \mid X(\omega) = x\})$. We call $(p_x)_{x \in S}$ the \vocab{probability mass function} of $X$ (pmf) or the distribution of $X$.

If $(p_x)$ is Bernoulli, then we say that $X$ is a Bernoulli random variable, or that $X$ has the Bernoulli distribution.
If $(p_x)$ is geometric, then similarity we sau $X$ is a geometric random variable.

\begin{definition}[Independence]
	Suppose that $X_1, \dots, X_n$ are discrete random variables taking values in $S_1, \dots, S_n$. We say that $X_1, \dots, X_n$ are \vocab{independent} if $\PP(X_1 = x_1, \dots, X_n = x_n) =\PP(X_1 = x_1) \cdots \PP(X_n = x_n)$.
\end{definition}

\begin{example}
	Toss a $p$-biased coin $N$ times. 

	In this case, $\Omega = \{0, 1\}^N$, corresponding to tails and heads. Then for $w \in \Omega$, $p_w = \prod p^{\omega_k} (1 - p)^{1 - \omega_k}$, for $\omega = (\omega_1, \dots, \omega_N)$.

	Define $X_k(\omega) = \omega_k$ for every $k = 1, \dots, N$ and $\omega \in \Omega$. This is a discrete random variable, and gives the outcome of the $k$-th toss.

	We have $\PP(X_k = 1) = \PP(\omega_k = 1) = p$, and $\PP(X_k = 0) = \PP(\omega_k = 0) = 1- p$. So $X_k$ has the Bernoulli distribution with parameter $p$.

	We can show that $X_1, \dots, X_N$ are independent random variables. Let $x_1, \dots, x_N \in \{0, 1\}$. Then
	\begin{align*}
	\PP(X_n &= x_1, \dots, X_N = x_n) = \PP(\omega = (x_1, \dots, x_N)) \\
			&= \prod_{k=1}^N p^{x_k} (1 - p)^{1 - x_k} = \prod_{k = 1}^{N} \PP(X_k = x_k).
\end{align*}

We can define $S_N(\omega) = X_1(\omega) + \cdots + X_N(\omega)$, which is the number of heads in $N$ tosses. So $S_N : \Omega \rightarrow \{0, \dots, N\}$ and $\PP(S_N = k) = \binom{N}{k}p^k (1 - p)^{N - k}$. So $S_N$ has the Binomial distribution of parameters $N$ and $p$.

\end{example}

\section{Expectation}

For some probability space $(\Omega, \FF, \PP)$ and assuming that $\Omega$ is finite or countable. Let $X : \Omega \rightarrow \R$ be a discrete random variable. We say that $X$ is \vocab{non-negative} if $X \geq 0$.

\begin{definition}[Expectation]
	We define the expectation of $X \geq 0$ so that
	$$
	\EE[X] = \sum_{\omega} X(\omega) \cdot \PP(\{\omega\})
	$$
\end{definition}

Writing $\Omega_X = \{X(\omega) \mid \omega \in \Omega \}$, so $\Omega = \bigcup_{x \in \Omega_X} \{X = x\}$, we can write this in the form
$$
\EE[X] = \sum_{x \in \Omega_X} \sum_{\omega \in \{X = x\}} x \cdot \PP (\{\omega\}) = \sum_{x \in \Omega_x} x \cdot \PP(X = x).
$$

So the expectation of $X$ (or the average value) is an average of the values taken by $X$ with weights given by $\PP(X = x)$.

\begin{example}
	Suppose $X$ has the Binomial distribution with parameters $N$ and $p$ ($X \sim B(N, p)$). Then computing we have
	$$
	\EE[X]= \sum_{k = 0}^N k \cdot \PP(X = k) = Np.
	$$
\end{example}

\begin{example}
	Let $X$ be a Poisson random variable with parameter $\lambda > 0$. Then 
	$$
	\EE[X] = \lambda.
	$$
\end{example}

Now let $X$ be a general (not necessarily non-negative) discrete random variable. We define $X_+ = \max(X, 0)$ and $X_- = \max(-X, 0)$. Then $X = X_+ - X_-$. We can then define $\EE[X_+]$ and $\EE[X_-]$. Then if at least one of them are finite, we have
$$
	\EE[X] = \EE[X_+] - \EE[X_-].
$$
If both are infinite, then we say the expectation of $X$ is not defined. Whenever we write $\EE[X]$, it is assumed to be well defined. If $\EE[|X|] < \infty$, we say that $X$ is \vocab{integrable}.

When $\EE[X]$ is well defined, we again have that
$$
\EE[X] = \sum_{x \in \Omega_X} x \cdot \PP(X = x).
$$

\begin{proposition}[Properties of Exectation]
For some discrete random variable $X$, we have the following.
\begin{enumerate}[label=(\roman*)]
	\item If $X \geq 0$, then $\EE[X] \geq 0$.
	\item If $X \geq 0$ and $\EE[X] = 0$, then $\PP(X = 0) = 1$.
	\item If $c \in \R$, then $\EE[cX] = c\EE[X]$ and $\EE[c + X] = c + \EE[X]$.
	\item If $X$ and $Y$ are two random variables, then $\EE[X + Y] = \EE[X] + \EE[Y]$, where $X$ and $Y$ are both integrable.
	\item Suppose $c_1, \dots, c_n \in \R$ and $X_1, \dots, X_n$ are discrete random variables. Then
	$$
	\EE\left[\sum_{i = 1}^n c_i X_i\right] = \sum_{i = 1}^n c_i \EE[X_i].
	$$
\end{enumerate}
\end{proposition}

\end{document}

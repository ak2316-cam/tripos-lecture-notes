%  \documentclass[DIV=12, a4]{scrartcl}
%\documentclass[12pt, a5]{scrartcl}

% \documentclass[a4paper]{report}
% \usepackage[
% % fancytheorems, 
% noindent, 
% %spacingfix, 
% %noheader
% ]{vanilla}


\documentclass[a4paper]{scrreprt}
\usepackage[
fancytheorems, 
noindent, 
% %spacingfix, 
% %noheader,
fancyproofs
]{adam} 

\usepackage{tikz}

% \usepackage{subfig}

% \setcounter{chapter}{-1}

\title{Probability}
% \subtitle{Adam Kelly}
\author{Adam Kelly}
% \date{Michaelmas 2020}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
	
	% \vspace{2\baselineskip}
	% {\color{red} None of the notes here have been reviewed at all, and are just exactly what was taken down live in the lectures. I would turn around now and come back in a few days, when I have gone back, cleaned things up, fixed explanations and added some structure.}
	% \vspace{5\baselineskip}

	This set of notes is a work-in-progress account of the course `Probability', originally lectured by Dr Perla Sousi in Lent 2020 at Cambridge. These notes are not a transcription of the lectures, but they do roughly follow what was lectured (in content and in structure).

	These notes are my own view of what was taught, and should be somewhat of a superset of what was actually taught. I frequently provide different explanations, proofs, examples, and so on in areas where I feel they are helpful. Because of this, this work is likely to contain errors, which you may assume are my own. If you spot any or have any other feedback, I can be contacted at \href{mailto:ak2316@cam.ac.uk}{ak2316@cam.ac.uk}.


	% {\color{red} Notes written upto lecture 6.}
	% During the creation of this document, I consulted a number of other books and resources. All of these are listed in the bibliography. 

\end{abstract}

\tableofcontents

\clearpage

\chapter{Basic Concepts}

Most of the phenomena in everyday lives involve randomness. What we try to do in probability is model this randomness in a mathematical way. It's likely that you have studied some probability before, but the difference in the treatment here is that we will try to be somewhat more rigerous.

We will define the notion of a probability space, where `our experiments take place'. Then we will discuss discrete and continuous random variables. 
In the discrete setting, we will find that there is no real subtleties, and we can be quite rigorous. In the continuous setting however we will have to take some things for granted (but rigour will return in the Part II course).

\begin{quote}
	\emph{``Probability theory has a right and a left hand. On the right is the rigorous foundational work using the tools of measure theory. The left hand `thinks probabilistically,' reduces problems to gambling situations, coin-tossing, motions of a physical particle.''}
\end{quote}

In this course, we will need both hands.

\section{Probability Space}


Probability is the mathematical formulation of randomness. So in order to study random phenomena in a rigorous way, we first need to set out a rigorous mathematical framework. 

The first notion that we will define is that of a \emph{probability space}.

\begin{definition}[$\sigma$-Algebra]
Suppose $\Omega$ is a set and $\FF$ is a collection of subsets of $\Omega$. We call $\FF$ a \vocab{$\sigma$-algebra} if the following properties are satisfied.
\begin{enumerate}[label=(\roman*)]
	\item $\Omega \in \FF$.
	\item If $A \in \FF$, then $A^c \in \FF$, the compliment of $A$.
	\item For any countable collection $A_1, A_2, \dots$ with $A_i \in \FF$ for all $i$, we must also have that $\bigcup_{i \geq 1} A_i \in \FF$.
\end{enumerate}
\end{definition}

\begin{definition}[Probability Measure]
	Suppose that $\FF$ is a $\sigma$-algebra on $\Omega$. Then a function $\PP : \FF \rightarrow [0, 1]$	is called a \vocab{probability measure} if the following are true.
	\begin{enumerate}[label=(\roman*)]
		\item $\PP(\Omega) = 1$.
		\item For any countable disjoint collection $A_1, A_2, \dots$ with $A_i \in \FF$ for all $i$, we have
		$$
		\PP\left(\bigcup_{n \geq 1} A_n\right) = \sum_{n \geq 1} \PP (A_n).
		$$
	\end{enumerate}
	We say that $\PP(A)$ is the \vocab{probability} of $A$.
\end{definition}

\begin{definition}[Probability Space]
	If $\Omega$ is a $\sigma$-algebra on $\Omega$ and $\PP$ is a probability measure, then $(\Omega, \FF, \PP)$ is a \vocab{probability space}.
\end{definition}

When the set $\Omega$ is countable, we take $\FF$ to be all subsets of $\Omega$.

\begin{definition}[Outcomes and Events]
	The elements of $\Omega$ are called \vocab{outcomes}, and the elements of $\FF$ are called \vocab{events}.
\end{definition}

Note that $\PP$ is defined on $\FF$, so it is defined on the \emph{events}, not the \emph{outcomes}. 

Let's look at some properties of the probability measure (which follow immediately from the definition).

\begin{proposition}[Properties of $\PP$]
	If $\PP$ is a probability measure then
	\begin{itemize}
		\item $\PP(A^c) = 1 - \PP(A)$
		\item $\PP(\emptyset) = 0$
		\item If $A \subset B$, then $\PP(A) \leq \PP(B)$
		\item $\PP(A \cup B) = \PP(A) + \PP(B) - \PP(A \cap B)$
	\end{itemize}
\end{proposition}
\begin{proof}[Proof Sketch] Check definitions.
\end{proof}

\begin{example}[Examples of Probability Spaces]
	Some examples of probability spaces are given below.
	\begin{itemize}
		\item \emph{Rolling a fair die}. Consider rolling a fair die. Then
		$
		\Omega = \{1, 2, 3, 4, 5, 6\}
		$, and $\FF$ is all subsets of $\Omega$.
		Then $\PP(\{w\}) = \frac{1}{6}$ for all $\omega \in \Omega$ and if $A \subseteq \Omega$, then $\PP(A) = \frac{|A|}{6}$.
		\item \emph{Equally likely outcomes}. Let $\Omega$ be a finite set, $\Omega = \{\omega_1, \omega_2, \dots, \omega_n \}$, and $\FF$ be all subsets of $\Omega$. Then define $\PP : \FF \rightarrow [0, 1]$ by $\PP(A) = \frac{|A|}{|\Omega|}$. In classical probability, this models picking a random element of $\Omega$. Note that $\PP(\{\omega_i\}) = \frac{1}{|\Omega|}$ for all $\omega_i \in \Omega$.
		\item \emph{Picking balls from a bag}. Suppose we have $n$ balls with $n$ labels from $\{1, \dots, n\}$ that are all indistinguishable by touch. Picking $k \leq n$ balls at random\footnote{That is, with all outcomes equally likely.} without replacement. Then we take $\Omega = \{A \subseteq \{1, 2, \dots, n\} \mid |A| = k \}$, and $\FF$ be all subsets of $\Omega$. Then $|\Omega| = \frac{n}{k}$, and for $\omega \in \Omega$, $\PP(\{\omega\}) = \frac{1}{|\Omega|}$.
		\item \emph{Deck of cards}. Take a well-shuffled deck of 52 cards. Then let $\Omega$ be the set of all permutations of the cards, and note $|\Omega| = 52!$. Then we have $\PP(\text{top 2 cards are aces}) = \frac{4 \cdot 3 \times 50!}{52!} = \frac{1}{221}$.
		\item \emph{Largest digit}. Consider a string of $n$ random digits from $0, \dots, 9$. Then $\Omega = \{0, 1, \dots, 9\}^n$, and $|\Omega| = 10^n$. Now define $A_k = \{\text{no digit exceeds }l\}$ and $B_k = \{\text{largest digit is }k\}$. Then $\PP(B_k) = \frac{|B_k|}{|\Omega|}$. Notice that $B_k = A_k \backslash A_{k - 1}$, and $|A_k| = (k + 1)^n$, so $|B_k| = (k + 1)^n - k^n$, thus $\PP(B_k) = \frac{(k + 1)^n - k^n}{10^n}$.
		\item \emph{Birthday Problem}. There are $n$ people. What is the probability that at least two of them share the same birthday? We can assume nobody is born on $29/02$, and that each birthday is equally likely. So $\Omega = \{1, \dots, 365\}^n$, and $\FF$ is all subsets of $\Omega$. As we assumed all outcomes are equally likely, we take $\PP(\{\omega\}) = \frac{1}{365^n}$ with $\omega \in \Omega$.
		Letting $A = \{\text{at least 2 people share a birthday}\}$, then $A^c = \{\text{all $n$ birthdays are different}\}$, and since $\PP(A) = 1 - \PP(A^c)$, it suffices to calculate $\PP(A^c)$. Now $\PP(A^c) = \frac{|A^c|}{|\Omega|} = \frac{365 \times 364 \times \cdots \times (365 - n + 1)}{365^n}$, and hence $\PP(A) = 1 - \frac{365 \times 364 \times \cdots \times (365 - n + 1)}{365^n}$. If $n = 23$, then this probability is approximately $0.507$.
\end{itemize}
\end{example}

\section{Combinatorial Analysis}

Suppose we have some finite set $\Omega$, and that $|\Omega| = n$. We want to partition $\Omega$ into $k$ disjoint subsets $\Omega_1, \Omega_2, \dots, \Omega_k$ with $|\Omega_i| = n_i$ and $\sum_{i = 1}^k n_i = n$. How many ways is there to do this?

If $M$ is the number of ways, then
$$
M = \binom{n}{n_1} \binom{n - n_1}{n_2} \cdots \binom{n - (n_1 + \cdots + n_{k - 1})}{n_k} = \frac{n!}{n_1! n_2 !\cdots n_k !}.
$$

\begin{definition}[Multinomial Coefficient]
	We define the \vocab{multinomial coefficient} $\binom{n}{n_1, \dots, n_k}$ to be the number of ways of partitioning a set with $n$ elements into $k$ subsets of size $n_1, n_2, \dots, n_k$. We have
	$$
	\binom{n}{n_1, \dots, n_k} = \frac{n!}{n_1! n_2 !\cdots n_k !}.
	$$
\end{definition}
	
Now let's think about the following question: How many strictly increasing and increasing functions are there between two sets?

If we have $f:\{1, \dots, k\} \rightarrow \{1, \dots, n\}$, we say it's \emph{strictly increasing} if whenever $x < y$, then $f(x) < f(y)$. We say that it's \emph{increasing} if $x < y$ implies $f(x) \leq f(y)$.

Any such function is uniquely determined by its range which is a subset of $\{1, \dots, n\}$ of size $k$. There are $\binom{n}{k}$ such subsets, and hence $\binom{n}{k}$ strictly increasing functions.

We define a bijection from $\{f : \{1, \dots, k \} \rightarrow \{1, \dots, n\} \mid f \text{ increasing}\}$ to $\{g : \{1, \dots, k \} \rightarrow \{1, \dots, n + k - 1\} \mid f \text{ strictly increasing}\}$. For each $f$, we define $g(i) = g(i) + i - 1$. Then $g$ is strictly increasing and takes values in $\{1, \dots, n + k - 1\}$. Then $g$ is strictly increasing and takes values in $\{1, \dots, n + k - 1\}$.

So the total number of increasing functions $f: \{1, \dots, k\} \rightarrow \{1, \dots, n\}$ is $\binom{n + k - 1}{k}$.

\section{Stirling's Formula}

Frequently in probability it will help to have some bounds/an asymptotic expression for the factorial.

\begin{notation}
Let $(a_n)$ and $(b_n)$ be two sequences. We will write $a_n \sim b_n$ if $\frac{a_n}{b_n} \rightarrow 1$ as $n \rightarrow \infty$.
\end{notation}

We will first prove a weak approximation of the factorial.

\begin{proposition}
	$\log(n!) \sim n \log n$ as $n \rightarrow \infty$.
\end{proposition}
\begin{proof}
	For $x \in \R$, we write $\lfloor x \rfloor$ for the \emph{integer part} of $x$. Then we have $\log\lfloor x \rfloor \leq \log x \leq \log \lfloor x + 1 \rfloor$. Integrating this from $1$ to $n$, we get
	\begin{align*}
		\sum_{k = 1}^{n - 1} \log k &\leq \int_1^n \log x \; \mathrm{d} x \leq \sum_{k = 1}^n \log k\\
\implies \log (n - 1)! &\leq n \log n - n + 1 \leq \log n!
	\end{align*}
	Using this, we get that
	$$
	n \log n - n + 1 \leq \log n! \leq (n + 1) \log (n + 1) - (n + 1) + 1,
	$$
	and dividing through by $n \log n$ we get
	$$
	\frac{\log n!}{n \log n} \rightarrow 1 \quad \quad n \rightarrow \infty.
	$$
\end{proof}

Now let's prove Stirling's formula. Note that the proof is non-examinable.


\begin{theorem}[Stirling Approximation]
	$n! \sim n^n \sqrt{2 \pi n} \; e^{-n}$ as $n \rightarrow \infty$.
\end{theorem}
\begin{proof}[Proof (Non-Examinable).]
	For any function $f$ that is twice differentiable, and $a < b$, then
	$$
	\int_a^b f(x) \; \mathrm{d}x = \frac{f(a) + f(b)}{2} (b - a) - \frac{1}{2} \int_a^b (x - a)(b - x)f''(x) \; \mathrm{d}x.
	$$
	This follows by integrating by parts.

	Now take $f(x) = \log x$, and $a = k$, $b = k + 1$. Then substituting into the formula, we get
	\begin{align*}
		\int_k^{k + 1} \log x \; \mathrm{d}x &= \frac{\log k + \log(k + 1)}{2} - \frac{1}{2} \int_k^{k + 1} \frac{(x - k)(k + 1 - x)}{x^2} \; \mathrm{d}x 	 \\
		&= \frac{\log k + \log (k + 1)}{2} + \frac{1}{2} \int_0^1 \frac{x(1 - x)}{(x + k)^2} \; \mathrm{d}x.
	\end{align*}
	Then taking the sum for $k = 1, \dots, n - 1$ of the equality above, we get
	\begin{align*}
		\int_1^n \log x \; \mathrm{d}x &= \frac{\log(n - 1)! + \log n!}{2} + \frac{1}{2} \sum_{k = 1}^{n - 1} \int_0^1 \frac{x (1 - x)}{(x + k)^2} \; \mathrm{d}x \\
\implies \log n-n+1 &= \log (n !)-\frac{\log n}{2}+\sum_{k=1}^{n-1} a_{k},
	\end{align*}
	where we set
	$$
	a_{k}=\frac{1}{2} \int_{0}^{1} \frac{x(1-x)}{(x+k)^{2}} \; \mathrm{d} x.
	$$
	But then
	\begin{align*}
		\log n ! &=n \log n-n+\frac{\log n}{2}+1-\sum_{k=1}^{n-1} a_{k} \\
\implies n !&=n^{n} e^{-n} \cdot \sqrt{n} \exp \left(1-\sum_{k=1}^{n-1} a_{k}\right) .
	\end{align*}
	Note that $a_{k} \leqslant \frac{1}{2} \int_{0}^{1} \frac{x(1-x)}{k^{2}} d x=\frac{1}{12 k^{2}}$, so $\sum a_k < \infty$. We set $A=\exp \left(1-\sum_{k=1}^{\infty} a_{k}\right) $. Then
	$$
	n !=n^{n} \cdot e^{-n} \sqrt{n} \cdot A \cdot \exp \left(\sum_{k=n}^{\infty} a_{k}\right),
	$$
	and as $\exp \left(\sum_{k=n}^{\infty} a_{k}\right) \rightarrow 1$ (since the argument goes to 0), we have proved that
	$$
	\frac{n!}{n^n e^{-n} \sqrt{n}} \rightarrow A, \quad \quad \text{as }n \rightarrow \infty,
	$$
	which means that $n! \sim n^n e^{-n} \sqrt{n} \cdot A$ as $n \rightarrow \infty$.

	To finish the proof, we need to show that $A = \sqrt{2 \pi}$.
	Knowing that $n! \sim n^n e^{-n} \sqrt{n} \cdot A$ as $n \rightarrow \infty$, we have
	$$
2^{-2n} \cdot \binom{2n}{n} = 2^{-2n} \frac{(2n)!}{n!\cdot n!} \sim \frac{2^{-2n} \cdot (2n)^{2n} \cdot \sqrt{2n} \cdot A \cdot e^{-2n}}{n^n \cdot e^{-n}\cdot \sqrt{n} \cdot A \cdot n^n \cdot e^{-n} \cdot \sqrt{n} \cdot A} = \frac{\sqrt{2}}{A \sqrt{n}}.
	$$
	Using a different method we will prove that
	$$
	2^{2n} \binom{2n}{n} \sim \frac{1}{\sqrt{\pi n}},
	$$
	which will force $A = \sqrt{2 \pi}$.

	Consider the integral
	$$
	I_n = \int_0^{2 \pi} (\cos \theta)^n \; \mathrm{d}\theta, \quad \quad n \geq 0.
	$$
	So $I_0 = \pi/2$ and $I_1 = 1$. Then integrating by parts, we get $I_n = \frac{n - 1}{n} T_{n - 2}$, and thus
	$$
	I_{2n} = \frac{2n - 1}{2n} \cdot I_{2n - 2} = \frac{(2n - 1)(2n - 3)\cdots 3 \cdot 1}{2n \cdot (2n - 2) \cdots 2} I_0 = \frac{(2n)!}{2^{2n} n! \cdot n!} \cdot \frac{\pi}{2},
	$$
	so
	$$
	I_{2n} = 2^{-2n} \binom{2n}{n} \frac{\pi}{2}.
	$$
	In the same way we get
	$$
	I_{2n + 1} = \frac{2n \cdots 4 \cdot 2}{(2n + 1) \cdots 3 \cdot 1} I_1 = \frac{1}{2n + 1} \left(2^{-2n} \binom{2n}{n}\right)^{-1}.
	$$
	From $I_n = \frac{n - 1}{n}I_{n - 2}$ we get $\frac{I_n}{I_{n - 2}}\rightarrow 1$ as $n \rightarrow \infty$, and what we want is
	$\frac{I_{2n}}{I_{2n + 1}} \rightarrow 1$ as $n \rightarrow \infty$.

	Note that $I_n$ is a decreasing function of $n$, therefore
	$$
	\frac{I_{2n}}{I_{2n + 1}} \leq \frac{I_{2n - 1}}{I_{2n + 1}} \rightarrow 1,
	$$
	and also
	$$
	\frac{I_{2n}}{I_{2n + 1}} \geq \frac{I_{2n}}{I_{2n - 2}} \rightarrow 1,
	$$
	thus $\frac{I_{2n}}{I_{2n+ 1}} \rightarrow 1$ as $n \rightarrow \infty$, which means
	\begin{align*}
		\frac{2^{-2n} \binom{2n}{n} \frac{\pi}{2}}{\left(2^{-2n} \binom{2n}{n}\right)^{-1} \frac{1}{2n + 1}} &\rightarrow 1 \\
\implies \left(2^{-2n} \binom{2n}{n}\right)^{2} \frac{\pi}{2} (2n + 1) \rightarrow 1,
	\end{align*}
	thus
	$$
	\left(2^{-2n} \binom{2n}{n}\right)^{2} \sim \frac{2}{\pi (2n + 1)} \sim \frac{1}{\pi n},
	$$
	thus $A = \sqrt{2 \pi}$, which completes the proof.
\end{proof}

\section{Properties of Probability Measures}

Let $(\Omega, \FF, \PP)$ be a probability space. Recall that the probability measure $\PP$ is a function
$$
\PP : \FF \rightarrow [0, 1]
$$
with $\PP(\Omega) = 1$ which is \emph{countable additive}, that is, for any countable disjoint collection $A_1$, $A_2$, $\dots$ with $A_i \in \FF$ for all $i$,
$$
\mathbb{P}\left(\bigcup_{n \geq 1} A_{n}\right)=\sum_{n \geq 1} \mathbb{P}\left(A_{n}\right).
$$

\subsection{Countable Subadditivity}

When the sequence is not necessarily disjoint, this equality becomes an inequality. 

\begin{proposition}[Countable Subadditivity]
	Let $(A_n)$ be a sequence of events with $A_i \in \FF$ for all $i$. Then we have
	$$
	\mathbb{P}\left(\bigcup_{n \geq 1} A_{n}\right) \leq \sum_{n \geq 1} \mathbb{P}\left(A_{n}\right).
	$$
\end{proposition}
\begin{proof}
	Define $B_1 = A_1$ and $B_n = A_n \backslash (A_1 \cup \cdots \cup A_{n - 1})$ for all $n \geq 2$. Then $(B_n)$ is a disjoint sequence of events in $\FF$, and $\bigcup_{n \geq 1} B_n = \bigcup_{n \geq 1} A_n$. So $\PP(\bigcup A_n) = \PP(\bigcup B_n)$. By countable additivity for $(B_n)$,
	$$
	\PP \left(\bigcup_{n \geq 1} B_n\right) = \sum_{n \geq 1} \PP(B_n).
	$$
	But $B_n \subseteq A_n$, so $\PP(B_n) \leq \PP(A_n)$ for all $n$. Therefore
	$$
	\PP(\bigcup A_n) = \PP(\bigcup B_n) = \sum \PP(B_n) \leq \sum_{n \geq 1} \PP(A_n).
	$$
\end{proof}


\end{document}

\chapter{Vectors in a General Setting, $\R^n$ and $\C^n$}

In the previous chapter, we studied vectors as they occur when considering position vectors in three dimensions. However, vectors are more general objects, which we will see in this chapter. First, we will consider how to immediately generalise vectors from $\R^3$ (as in the previous chapter) to $\R^n$.

\section{Vectors in $\R^n$}

By regarding vectors as sets of components, it's easy to generalize from 3 to $n$ dimensions.

\begin{definition}[$\R^n$ Space]
    We define $\R^n$ to be the set of $n$-tuples of real numbers,
    $$
    \R^n = \{(x_1, x_2, \dots, x_n) \mid x_i \in \R\}.
    $$ 
    We also define \vocab{vector addition} and \vocab{scalar mulitplication} as
    \begin{align*}
        (x_1, \dots, x_n) + (y_1, \dots, y_n) &= (x_1 + y_1, \dots, x_n + y_n),\\
       \text{and} \quad \lambda (x_1, \dots, x_n) &= (\lambda x_1, \dots, \lambda x_n).
    \end{align*}
\end{definition}

As before, we can take arbitrary linear combinations and we also get a notion of parallelism, where $\vv{x} \parallel \vv{y}$ if and only if $\vv{x} = \lambda \vv{y}$ or $\vv{y} = \lambda \vv{x}$ for $\lambda \in \R$.

We also still have a dot product, but in more general settings, such operations are normally referred to as \emph{inner products}.

\begin{definition}[Inner Product]
    The inner product on $\R^n$ is defined by
    $$
    \vv{x} \cdot \vv{y} = \sum_{i} x_i y_i = x_1 y_1 + x_2 y_2 + \cdot + x_n y_n.
    $$
\end{definition}

\begin{proposition}[Properties of the Inner Product]
    In $\R^n$, the inner product satisfies
    \begin{enumerate}[label=(\roman*)]
        \item \emph{Symmetric}. $\vv{x} \cdot \vv{y} = \vv{y} \cdot \vv{x}$.
        \item \emph{Bilinear}. $(\lambda \vv{x} + \lambda' \vv{x}') \cdot \vv{y} = \lambda \vv{x} \cdot \vv{y} + \lambda' \vv{x}' \cdot \vv{y}$.
        \item \emph{Positive Definite}. $\vv{x} \cdot \vv{x} = \sum_{i} x_i ^2 \geq 0$, and is equal 0 if and only if $\vv{x} = \vv{0}$. 
    \end{enumerate}
\end{proposition}

Also, if we return back to vectors in three-dimensions, we had an equivalent definition of the dot product as
$$
\vv{x} \cdot \vv{y} = |\vv{x}| |\vv{y}| \cos \theta,
$$
where $\theta$ is the angle between the vectors. Using this as a definition of the inner product in $\R^n$, we get a definition for the angle between vectors.

When we look at vector spaces in general, we will define the inner product in general to be any operation that satisfies these properties.

We also have the notions of length and orthogonality, much the same as in three-dimensions.

\begin{definition}[Length/Norm]
    For $\vv{x} \in \R^n$, we define its \vocab{length} or \vocab{norm} to be $|\vv{x}| \geq 0$ with $|\vv{x}|^2 = \vv{x} \cdot \vv{x}$.
\end{definition}

\begin{definition}[Orthogonality]
    We say two vectors $\vv{x}, \vv{y} \in \R^n$ are \vocab{orthogonal} if and only if $\vv{x} \cdot \vv{y} = 0$.
\end{definition}

When working in $\R^n$, we typically use a set of basis vectors known as the \vocab{standard basis}. They are
\begin{align*}
    \vv{e}_1 &= (1, 0, 0, \dots, 0), \\
    \vv{e}_2 &= (0, 1, 0, \dots, 0), \\
        &\vdots \\
        \vv{e}_n &= (0, 0, 0, \dots, 1).
\end{align*}
Then we have
$$
\vv{x} = \sum_i x_i \vv{e}_i,
$$
and $\vv{e}_i \cdot \vv{e}_j = \delta_{ij}$, so thus basis is orthonormal.

\subsection{Cauchy-Schwarz and the Triangle Inequality}

We will now state an inequality about the inner product of two vectors in $\R^n$.

\begin{proposition}[Cauchy-Schwarz Inequality]
    For all vectors $\vv{x}, \vv{y} \in \R^n$, we have
    $$
|\vv{x} \cdot \vv{y}| \leq |\vv{x} | \cdot |\vv{y}|,
    $$
    with equality if and only if $\vv{x} \parallel \vv{y}$.
\end{proposition}
\begin{proof}
    If $\vv{y} = \vv{0}$, then we are done. Otherwise, consider
    \begin{align*}
        |\vv{x} - \lambda \vv{y}|^2 &= (\vv{x} - \lambda\vv{y}) \cdot (\vv{x} - \lambda \vv{y}) \\
        &= |\vv{x}|^2 - 2 \lambda \vv{x} \cdot \vv{y} + \lambda^2 |\vv{y}|^2 \geq 0
    \end{align*}
    for any $\lambda \in \R$. This is a real quadratic in $\lambda$ with at most one real root. So it's discriminant is
    $$
    (-2 \vv{x} \cdot \vv{y})
^2 - 4|\vv{x}|^2 |\vv{y}|^2 \leq 0,    $$
so equality holds if and only if the discriminant is zero, that is, if $\vv{x} = \lambda \vv{y}$, as required.
\end{proof}

As vectors in $\R^n$ are just sets of components, we can state this inequality in terms of real numbers.
    For a sequence of real numbers $(a_i)$ and $(b_i$), we have
    $$
\left(\sum_{i = 1}^n a_i b_i\right)^2 \leq \left(\sum_{i = 1}^n a_i^2 \right)\left(\sum_{i = 1}^n b_i^2 \right),
    $$
    where we have equality if and only if $\frac{a_i}{b_i}$ is a constant for all $i$ where $a_i b_i \neq 0$.

We also have the triangle inequality, similar to the one stated in Chapter 1.

\begin{proposition}[Triangle Inequality]
    For all $\vv{x},\vv{y} \in \R^n$,
    $$
    |\vv{x} + \vv{y} |\leq |\vv{x}| + |\vv{y}|
    $$
\end{proposition}
\begin{proof}
We have
\begin{align*}
    |\vv x + \vv y|^2 &= |\vv x|^2 + 2 \vv{x}\cdot \vv{y} + |\vv{y}|^2 \\
    &\leq |\vv{x}|^2 + 2 |\vv{x}| \cdot |\vv{y}| + |\vv{y}|^2 \\
    &= (|\vv{x}| + |\vv{y}|)^2,
\end{align*}
as required.
\end{proof}

\subsection{A comment about $\R^n$ and $\R^3$}

The definition we gave for the inner product on $\R^n$ can be written as
$$
\vv{a} \cdot \vv{b} = \delta_{ij} a_i b_j,
$$
for $\vv{a}, \vv{b} \in \R^n$ where we use the summation convention. For $n = 3$, this matches with the compact definition of the dot product we gave previously.

However, in three dimensions we also had a compact definition of the cross product with
$$
(\vv{a} \times \vv{b})_i = \varepsilon_{ijk} a_j b_k.
$$
In $\R^n$ however, we have $\varepsilon_{ij\dots k}$ where there is $n$ indices (we will look at this again in Chapter 5). This is totally antisymmetric, so we cannot use $\varepsilon$ to define a vector-valued product for any $n$ except $3$. 

In $\R^2$ however, we have $\varepsilon_{ij}$ where $\varepsilon_{12} = - \varepsilon_{21} = 1$, and we can define an additional scalar product
$$
[\vv{a}, \vv{b}] = \varepsilon_{ij}a_i b_j = a_1 b_2 - a_2 b_1.
$$
This corresponds with the signed area of a parallelogram, and $|[\vv{a}, \vv{b}]| = |\vv{a}| |\vv{b}| \sin \theta$. Compare this with $[\vv{a}, \vv{b}, \vv{c}] = \varepsilon_{ijk}a_i b_j c_k$, the signed volume of the parallelepiped in three dimensions we defined earlier.

\section{Vector Spaces}

We will now go one step further and define what a vector is in a much more general sense.
This will be done by defining the properties that vectors have, rather than what they are themselves.

\begin{definition}[Vector Space]
  Let $V$ be a set of objects called \vocab{vectors} with operations $\vv{v} + \vv{w} \in V$ defined for all $\vv{v}, 
\vv{w} \in V$, and $\lambda \vv{v} \in V$ for all $\vv{v} \in V$ and $\lambda \in F$, where $F$ is some field. 

Then $V$ is a \vocab{vector space} if
\begin{enumerate}[label=(\roman*)]
    \item $V$ with $+$ is an abelian group.
    \item $\lambda( \vv{v} + \vv{w}) = \lambda \vv{v} + \lambda \vv{w}$.
    \item $(\lambda  + \mu)\vv{v} = \lambda \vv{v} + \mu \vv{v}$.
    \item $\lambda (\mu \vv{v}) = (\lambda \mu) \vv{v}$.
    \item $1 \vv{v} = \vv{v}$.
\end{enumerate}
\end{definition}

In this course, we will deal only with the fields $\R$ and $\C$. If $F = \R$, then we call $V$ a \vocab{real vector space}, and if $F = \C$, then we call it a \vocab{complex vector space}.

\begin{example}[Example of a Real Vector Space]
  Let $V$ be the set of all functions $f: [0, 1] \rightarrow \R$ where $f(0) = f(1) = 0$ and $f$ is smooth. Then $V$ is a real vector space.

  Indeed we have
  \begin{align*}
      (f + g)(x) &= f(x) + g(x), \\
      (\lambda f)(x) &= \lambda (f(x)).
  \end{align*}
  The other properties can be checked as required.
\end{example}

We can also define a \emph{subspace} in the natural way.

\begin{definition}[Subspace]
    A \vocab{subspace} of a vector space $V$ is a subset $U \subseteq V$ that is also a vector space.
\end{definition}

Note that a non-empty subset is a subspace if and only if $\vv{v}, \vv{w} \in U$ implies $\lambda \vv{v} + \mu \vv{w} \in U$ where $\lambda, \mu$ are some arbitrary scalars.

For any vectors $\vv{v}_1, \vv{v}_2, \dots, \vv{v}_r \in V$, their span
$$
    \vecspan \{\vv{v}_1, \dots, \vv{v}_r\} = \{ \lambda_1 \vv{v}_1 + \cdots + \lambda_r \vv{v}_r \mid \lambda_i \in \R \text{ or } \C \}.
$$
is a subspace. Also note that $V$ and $\{ \vv{0} \}$ are both subspaces of a vector space $V$.

\subsection{Linear Dependance \& Independence}

We will now think about whether sets of vectors are independent. 

\begin{definition}[Linear Dependance/Independence]
    For $\vv{v}_1, \vv{v}_2, \dots, \vv{v}_r \in V$, some vector space, consider the \vocab{linear relation}
    \begin{equation}\label{eq:dag}
        \lambda_1 \vv{v}_1 + \lambda_2 \vv{v}_2 + \cdots + \lambda_r \vv{v}_r = \vv{0} \tag{$\dagger$}
    \end{equation}
    If \eqref{eq:dag} implies that $\lambda_i = 0$ for all $i$, then we these vectors form a \vocab{linearly independent} set. If \eqref{eq:dag} holds with some $\lambda_i \neq 0$, then they form a \vocab{linearly dependant} set.
\end{definition}


\begin{example}[Example of a Linearly Dependent Set]
    In $\R^3$, the set $\left\{\begin{pmatrix}1 \\ 0\end{pmatrix}, \begin{pmatrix}0 \\ 1\end{pmatrix}, \begin{pmatrix}0 \\ 2\end{pmatrix}\right\}$ is linearly dependent, since
    $$
    0\begin{pmatrix}1 \\ 0\end{pmatrix} + 2\begin{pmatrix}1 \\ 0\end{pmatrix} - 1\begin{pmatrix}1 \\ 0\end{pmatrix} = \begin{pmatrix}0 \\ 0\end{pmatrix},
    $$
    which is a non-trivial linear relation.
\end{example}

Any set containing $\vv{0}$ is linearly dependent, since we can $\lambda \vv{0} = \vv{0}$ for any $\lambda$.

In $\R^3$, the set of vectors $\{\vv{a}, \vv{b}, \vv{c}\}$ are linearly independent if $\vv{a} \cdot (\vv{b} \times \vv{c}) \neq \vv{0}$, since if $\alpha \vv{a} + \beta \vv{b} + \gamma \vv{c} = \vv{0}$, then dotting with $\vv{b} \times \vv{c}$ gives $\alpha \vv{a} \cdot (\vv{b} \times \vv{c}) = 0 \implies \alpha = 0$, and $\beta, \gamma$ similarly.


\subsection{Inner Products}

A real vector space can have an additional structure that we can define by axioms.

\begin{definition}[Inner Product]
    For a real vector space $V$ and $\vv{v}, \vv{w} \in V$, an \vocab{inner product} denoted by
    $$
    \vv{v} \cdot \vv{w} \quad \text{or} \quad (\vv{v}, \vv{w}) \in \R
    $$
    is a map that is symmetric, bilinear, and positive definite.
\end{definition}

If these conditions hold, then we we get things like the Cauchy-Schwartz inequality and a norm as by the previous argument.

\begin{example}[Inner Product on a Real Vector Space]
    Consider the real vector space of functions $V = \{ f: [0, 1] \rightarrow \R \mid f \text{ smooth, and} f(0) = f(1) = 0 \}$.
    We can define the inner product as
    $$
    (f, g) = \int_0^1 f(x)g(x) \; \mathrm{d}x,
    $$
    which satisfies the conditions specified above. Then we have
    $$
    ||(f, g)|| \leq ||f|| \cdot ||g||,
    $$
    that is,
    $$
    \left| \int_0^1 f(x)g(x)\; \mathrm{d}x \right| \leq \left(\int_0^1 f(x)^2\; \mathrm{d}x\right)^{1/2} \left(\int_0^1 g(x)^2\; \mathrm{d}x\right)^{1/2}.
    $$
\end{example}

We can also say something about linear independence from an inner product.

\begin{lemma}
In any real vector space $V$ with an inner product, if $\vv{v}_1, \vv{v}_2, \dots, \vv{v}_r$ are non-zero and orthogonal vectors, then they are linearly independent.
\end{lemma}
\begin{proof}
    If $\sum_i \alpha_i \vv{v}_i = \vv{0}$, then $(\vv{v}_j, \sum_i \alpha_i \vv{v}_i) = 0$ for a fixed $j$, so $\alpha_j (\vv{v}_j, \vv{v}_j) = 0$, and thus $\alpha_j = 0$.
\end{proof}

\section{Basis and Dimension}

We can generalize the notion of a basis to an arbitrary vector space.

\begin{definition}[Basis]
    For a vector space $V$, a \vocab{basis} is a set $B = \{\vv{e}_1, \vv{e}_2, \dots, \vv{e}_n \}$ such that
    \begin{enumerate}[label=(\roman*)]
        \item $B$ spans $V$, that is, any $\vv{v} \in V$ can be written $\vv{v} = \sum_{i = 1}^n v_i \vv{e}_i$.
        \item $B$ is linearly independent.
    \end{enumerate}
\end{definition}

The linear independence implies that the coefficients $v_i$ in (i) are unique. We say that $v_i$ are the components of $\vv{v}$ with respect to the basis $B$.

\begin{example}[Standard Basis]
    The standard basis for $\R^n$ consists of the vectors
    $$
\vv{e}_1 = \begin{pmatrix}1 \\ 0\\ \vdots \\ 0\end{pmatrix}, \quad \vv{e}_2 = \begin{pmatrix}0 \\ 1\\ \vdots \\ 0\end{pmatrix}, \quad \dots, \quad \vv{e}_n = \begin{pmatrix}0 \\ 0\\ \vdots \\ 1\end{pmatrix}.
    $$
    This is a basis since it satisfies the definition as $\vv{x} \in \R^n$, we have
    $$
\vv{x} = \begin{pmatrix}x_1 \\ x_2\\ \vdots \\ x_n\end{pmatrix} = x_1 \vv{e}_1 + \cdots + x_n \vv{e}_n,
    $$
    and $\vv{x} = \vv{0}$ if and only if $x_1 = x_2 = \cdots = x_n = 0$. 
\end{example}

Of course in the example above we just used one particular base. 
For example in $\R^2$ another basis consists of $\{(1, 0), (1, 1)\}$. 

\begin{theorem}[The Basis Theorem]
    If $\{\vv{e}_1, \dots, \vv{e}_m\}$ and $\{\vv{f}_1, \dots, \vv{f}_m\}$ are both bases, then $n = m$.
\end{theorem}
\begin{proof}
    We can find coefficients $A_{ai}$ and $B_{ia}$ such that $\vv{f}_a = \sum_i A_{ai} \vv{e}_i$ and $\vv{e}_i = \sum_a B_{ia} \vv{f}_a$. So we have that
    \begin{align*}
    \vv{f}_a &= \sum_i A_{ai} \left(\sum_b B_{ib} \vv{f}_b\right) \\
        &= \sum_b \left(\sum_{i} A_{ai} B_{ib}\right)\vv{f}_b,
\end{align*}
    but coefficients with respect to a basis are unique, hence
    $$
    \sum_{i} A_{ai} B_{ib} = \delta_{ab}.
    $$
    Similarily, $\sum_{a} B_{ia} A_{aj} = \delta_{ij}$. Finally,
    \begin{align*}
        \sum_{ia} A_{ai} B_{ia} &= \sum_{a} \delta_{aa} = m \\
        &= \sum_i \delta_ii = n,
     \end{align*}
     hence $n = m$.
\end{proof}

This theorem allows us to state the following definition.

\begin{definition}[Definition]
    The \vocab{dimension} of a vector space $V$, written $\operatorname{dim} V$, is the size of its basis.
\end{definition}

Note that $\R^n$ has dimension $n$.

The steps in the proof of the basis theorem are within the scope of the course, but you will not be expected to prove this theorem without prompts (it is non-examinable in the schedules). The same applies with the following.


\begin{proposition}
    Let $V$ be a vector space with finite subsets $Y = \{\vv{w}_1, \vv{w}_2, \dots, \vv{w}_m\}$ that spans $V$, and $X = \{\vv{u}_1, \dots, \vv{u}_k \}$ that is linearly independent. Then $k \leq n \leq m$ where $n = \operatorname{dim} V$. Also
    \begin{enumerate}[label=(\roman*)]
        \item A basis can be formed from subsets of $Y$,
        \item A basis can be obtained from $X$ by adding additional vectors (from $Y$ if necessary).
    \end{enumerate}
\end{proposition}
\begin{proof}
    \begin{enumerate}[label=(\roman*)]
        \item If $Y$ is linearly independent, then it's a basis and $m = n = \operatorname{dim} V$. If $Y$ is not linearly independent, then
        $$
        \sum_{i = 1}^m \lambda_i \vv{w}_i = \vv{0}, \quad \text{with some } \lambda_i \neq 0.
        $$
        Take $\lambda_m \neq 0$ without loss of generality, then $\vv{w}_m = -\frac{1}{\lambda_m} \sum_{i = 1}^{m - 1} \vv{w}_i$,
        so $\vecspan Y = \vecspan Y'$, with $Y' = \{\vv{w}_1, \dots, \vv{w}_{m - 1}\}$. This can be repeated until a basis is obtained.
        \item If $X$ spans $V$ then it's a basis and $k = n = \operatorname{dim} V$. If not, then there exists $\vv{u}_{k + 1} \in V$ not in $\vecspan X$. But then $\sum_{i = 1}^{k + 1} \mu_i \vv{u}_i = \vv{0}$ implies $\mu_{k + 1} = 0$, thus $X' = \{\vv{u}_1, \dots, \vv{u}_{k + 1} \}$ is linearly independent. Furthermore, we can choose $\vv{u}_{k + 1} \in Y$ as otherwise $Y \subseteq \vecspan X \implies \vecspan Y \subseteq \vecspan X \implies \vecspan X = V$.
        We can repeat this process until we obtain a basis (and this must stop since $Y$ is a finite set).   \qedhere
    \end{enumerate}
\end{proof}

Not every vector space has a finite dimension, but in this course we will deal only with vector spaces that do, save for the occasional example.

\begin{example}[Example of an Infinite Dimensional Vector Space]
    Consider the vector space
    $$
    V = \{ f : [0, 1] \rightarrow \R \mid \text{f smooth, } f(0) = f(1) = 0 \}. 
    $$
    Then consider $S_n(x) = \sqrt{2} \sin(n \pi x)$, $n = 1, 2, \dots$. These belong to $V$ and
    $$
    (S_n, S_m) = 2 \int_{0}^1 \sin (n \pi x) \sin (m \pi x) \; \mathrm{d}x = \delta_{mn},
    $$
    so they are all linearly independent. But we have infinitely many of them, so $V$ is infinite dimensional.
\end{example}

\section{Vectors in $\C^n$}

So far we have looked at real vector spaces, and particularly at $R^n$. We can define a similar vector space over the complex numbers.

\begin{definition}[$\C^n$ Space]
    We define $\C^n$ to be the set of $n$-tuples of complex numbers,
    $$
    \C^n = \{(z_1, z_2, \dots, z_n) \mid z_j \in \C\}.
    $$
    We also define \vocab{addition} and scalar multiplication with
    \begin{align*}
        \left(z_{1}, \ldots, z_{n}\right)+\left(w_{1}, \ldots, w_{n}\right)&=\left(z_{1}+w_{1}, \ldots, z_{n}+w_{n}\right) \\
        \text{and} \quad \lambda\left(z_{1}, \ldots, z_{n}\right)=\left(\lambda z_{1}, \ldots, \lambda z_{n}\right) 
    \end{align*}
\end{definition}

Note that we could have used real scalars and have gotten a real vector space, but having a complex vector space is more natural. 

The distinction between real and complex vector spaces is important. For example, if we have $\vv{z} = (z_1, \dots, z_n) \in \C^n$, with $z_j = x_j + i y_j$ then
$$
\vv{z} = \sum_j x_j \vv{e}_j + \sum_j y_i \vv{f}_i
$$
is a real linear combination, and thus $\{\vv{e}_1, \dots, \vv{e}_n, \vv{f}_1, \dots, \vv{f}_n\}$ is a basis for $\C^n$ as a real vector space, and it has dimension $2n$. However, taking $C^n$ as defined before, it is of dimension $n$ which is clearly different.

From now on we will view $\C^n$ as a complex vector space unless we say otherwise.

\subsection{Complex Inner Product}

We can define an inner product on $\C^n$.

\begin{definition}[Complex Inner Product]
    For $\vv{z}, \vv{w} \in \C^n$, we define the inner product to be
    $$
    (\vv{z}, \vv{w}) = \sum_j \overline{z_j} w_j = \overline{z_1} w_1 + \cdots + \overline{z_n} w_n.
    $$
\end{definition}

\begin{proposition}[Properties of the Complex Inner Product]
    The complex inner product satisfies
    \begin{enumerate}[label=(\roman*)]
        \item \emph{Hermitian}. $(\vv{w}, \vv{z}) = \overline{(\vv{z}, \vv{w})}$.
        \item \emph{Linear/anti-linear}. $(\vv{z}, \lambda \vv{w}+ \lambda' \vv{w}') = \lambda(\vv{z}, \vv{w}) + \lambda' (\vv{z}, \vv{w}')$ and $(\lambda \vv{z} + \lambda' \vv{z}', \vv{w}) = \overline{\lambda}(\vv{z}, \vv{w}) + \overline{\lambda'} (\vv{z}' , \vv{w})$.
        \item \emph{Positive definite}. $(\vv{z}, \vv{z}) = \sum_j |z_j|^2$, which is real, non-negative and zero if and only if $\vv{z} = \vv{0}$.
    \end{enumerate}
\end{proposition}
\begin{proof}[Proof Sketch]
    Check definitions.
\end{proof}

We also define \emph{length} or \emph{norm} of $\vv{z}$ to be $|\vv{z}| \geq 0$ with $|\vv{z}|^2 = (\vv{z}, \vv{z})$.

We can also say that $\vv{z}, \vv{w} \in \C^n$ are orthogonal if $(\vv{z}, \vv{w}) = 0$. With this, note that the standard basis for $\C^n$ is orthonormal.

\chapter{Vectors in a General Setting, $\R^n$ and $\C^n$}

In the previous chapter, we studied vectors as they occur when considering position vectors in three dimensions. However, vectors are more general objects, which we will see in this chapter. First, we will consider how to immediately generalise vectors from $\R^3$ (as in the previous chapter) to $\R^n$.

\section{Vectors in $\R^n$}

By regarding vectors as sets of components, it's easy to generalize from 3 to $n$ dimensions.

\begin{definition}[$\R^n$ Space]
    We define $\R^n$ to be the set of $n$-tuples of real numbers,
    $$
    \R^n = \{(x_1, x_2, \dots, x_n) \mid x_i \in \R\}.
    $$ 
    We also define \vocab{vector addition} and \vocab{scalar mulitplication} as
    \begin{align*}
        (x_1, \dots, x_n) + (y_1, \dots, y_n) &= (x_1 + y_1, \dots, x_n + y_n),\\
       \text{and} \quad \lambda (x_1, \dots, x_n) &= (\lambda x_1, \dots, \lambda x_n).
    \end{align*}
\end{definition}

As before, we can take arbitrary linear combinations and we also get a notion of parallelism, where $\vv{x} \parallel \vv{y}$ if and only if $\vv{x} = \lambda \vv{y}$ or $\vv{y} = \lambda \vv{x}$ for $\lambda \in \R$.

We also still have a dot product, but in more general settings, such operations are normally referred to as \emph{inner products}.

\begin{definition}[Inner Product]
    The inner product on $\R^n$ is defined by
    $$
    \vv{x} \cdot \vv{y} = \sum_{i} x_i y_i = x_1 y_1 + x_2 y_2 + \cdot + x_n y_n.
    $$
\end{definition}

\begin{proposition}[Properties of the Inner Product]
    In $\R^n$, the inner product satisfies
    \begin{enumerate}[label=(\roman*)]
        \item \emph{Symmetric}. $\vv{x} \cdot \vv{y} = \vv{y} \cdot \vv{x}$.
        \item \emph{Bilinear}. $(\lambda \vv{x} + \lambda' \vv{x}') \cdot \vv{y} = \lambda \vv{x} \cdot \vv{y} + \lambda' \vv{x}' \cdot \vv{y}$.
        \item \emph{Positive Definite}. $\vv{x} \cdot \vv{x} = \sum_{i} x_i ^2 \geq 0$, and is equal 0 if and only if $\vv{x} = \vv{0}$. 
    \end{enumerate}
\end{proposition}

Also, if we return back to vectors in three-dimensions, we had an equivalent definition of the dot product as
$$
\vv{x} \cdot \vv{y} = |\vv{x}| |\vv{y}| \cos \theta,
$$
where $\theta$ is the angle between the vectors. Using this as a definition of the inner product in $\R^n$, we get a definition for the angle between vectors.

When we look at vector spaces in general, we will define the inner product in general to be any operation that satisfies these properties.

We also have the notions of length and orthogonality, much the same as in three-dimensions.

\begin{definition}[Length/Norm]
    For $\vv{x} \in \R^n$, we define its \vocab{length} or \vocab{norm} to be $|\vv{x}| \geq 0$ with $|\vv{x}|^2 = \vv{x} \cdot \vv{x}$.
\end{definition}

\begin{definition}[Orthogonality]
    We say two vectors $\vv{x}, \vv{y} \in \R^n$ are \vocab{orthogonal} if and only if $\vv{x} \cdot \vv{y} = 0$.
\end{definition}

When working in $\R^n$, we typically use a set of basis vectors known as the \vocab{standard basis}. They are
\begin{align*}
    \vv{e}_1 &= (1, 0, 0, \dots, 0), \\
    \vv{e}_2 &= (0, 1, 0, \dots, 0), \\
        &\vdots \\
        \vv{e}_n &= (0, 0, 0, \dots, 1).
\end{align*}
Then we have
$$
\vv{x} = \sum_i x_i \vv{e}_i,
$$
and $\vv{e}_i \cdot \vv{e}_j = \delta_{ij}$, so thus basis is orthonormal.

\subsection{Cauchy-Schwarz and the Triangle Inequality}

We will now state an inequality about the inner product of two vectors in $\R^n$.

\begin{proposition}[Cauchy-Schwarz Inequality]
    For all vectors $\vv{x}, \vv{y} \in \R^n$, we have
    $$
|\vv{x} \cdot \vv{y}| \leq |\vv{x} | \cdot |\vv{y}|,
    $$
    with equality if and only if $\vv{x} \parallel \vv{y}$.
\end{proposition}
\begin{proof}
    If $\vv{y} = \vv{0}$, then we are done. Otherwise, consider
    \begin{align*}
        |\vv{x} - \lambda \vv{y}|^2 &= (\vv{x} - \lambda\vv{y}) \cdot (\vv{x} - \lambda \vv{y}) \\
        &= |\vv{x}|^2 - 2 \lambda \vv{x} \cdot \vv{y} + \lambda^2 |\vv{y}|^2 \geq 0
    \end{align*}
    for any $\lambda \in \R$. This is a real quadratic in $\lambda$ with at most one real root. So it's discriminant is
    $$
    (-2 \vv{x} \cdot \vv{y})
^2 - 4|\vv{x}|^2 |\vv{y}|^2 \leq 0,    $$
so equality holds if and only if the discriminant is zero, that is, if $\vv{x} = \lambda \vv{y}$, as required.
\end{proof}

As vectors in $\R^n$ are just sets of components, we can state this inequality in terms of real numbers.
    For a sequence of real numbers $(a_i)$ and $(b_i$), we have
    $$
\left(\sum_{i = 1}^n a_i b_i\right)^2 \leq \left(\sum_{i = 1}^n a_i^2 \right)\left(\sum_{i = 1}^n b_i^2 \right),
    $$
    where we have equality if and only if $\frac{a_i}{b_i}$ is a constant for all $i$ where $a_i b_i \neq 0$.

We also have the triangle inequality, similar to the one stated in Chapter 1.

\begin{proposition}[Triangle Inequality]
    For all $\vv{x},\vv{y} \in \R^n$,
    $$
    |\vv{x} + \vv{y} |\leq |\vv{x}| + |\vv{y}|
    $$
\end{proposition}
\begin{proof}
We have
\begin{align*}
    |\vv x + \vv y|^2 &= |\vv x|^2 + 2 \vv{x}\cdot \vv{y} + |\vv{y}|^2 \\
    &\leq |\vv{x}|^2 + 2 |\vv{x}| \cdot |\vv{y}| + |\vv{y}|^2 \\
    &= (|\vv{x}| + |\vv{y}|)^2,
\end{align*}
as required.
\end{proof}

\subsection{A comment about $\R^n$ and $\R^3$}

The definition we gave for the inner product on $\R^n$ can be written as
$$
\vv{a} \cdot \vv{b} = \delta_{ij} a_i b_j,
$$
for $\vv{a}, \vv{b} \in \R^n$ where we use the summation convention. For $n = 3$, this matches with the compact definition of the dot product we gave previously.

However, in three dimensions we also had a compact definition of the cross product with
$$
(\vv{a} \times \vv{b})_i = \varepsilon_{ijk} a_j b_k.
$$
In $\R^n$ however, we have $\varepsilon_{ij\dots k}$ where there is $n$ indices (we will look at this again in Chapter 5). This is totally antisymmetric, so we cannot use $\varepsilon$ to define a vector-valued product for any $n$ except $3$. 

In $\R^2$ however, we have $\varepsilon_{ij}$ where $\varepsilon_{12} = - \varepsilon_{21} = 1$, and we can define an additional scalar product
$$
[\vv{a}, \vv{b}] = \varepsilon_{ij}a_i b_j = a_1 b_2 - a_2 b_1.
$$
This corresponds with the signed area of a parallelogram, and $|[\vv{a}, \vv{b}]| = |\vv{a}| |\vv{b}| \sin \theta$. Compare this with $[\vv{a}, \vv{b}, \vv{c}] = \varepsilon_{ijk}a_i b_j c_k$, the signed volume of the parallelepiped in three dimensions we defined earlier.

\section{Vector Spaces}

We will now go one step further and define what a vector is in a much more general sense.
This will be done by defining the properties that vectors have, rather than what they are themselves.

\begin{definition}[Vector Space]
  Let $V$ be a set of objects called \vocab{vectors} with operations $\vv{v} + \vv{w} \in V$ defined for all $\vv{v}, 
\vv{w} \in V$, and $\lambda \vv{v} \in V$ for all $\vv{v} \in V$ and $\lambda \in F$, where $F$ is some field. 

Then $V$ is a \vocab{vector space} if
\begin{enumerate}[label=(\roman*)]
    \item $V$ with $+$ is an abelian group.
    \item $\lambda( \vv{v} + \vv{w}) = \lambda \vv{v} + \lambda \vv{w}$.
    \item $(\lambda  + \mu)\vv{v} = \lambda \vv{v} + \mu \vv{v}$.
    \item $\lambda (\mu \vv{v}) = (\lambda \mu) \vv{v}$.
    \item $1 \vv{v} = \vv{v}$.
\end{enumerate}
\end{definition}

In this course, we will deal only with the fields $\R$ and $\C$. If $F = \R$, then we call $V$ a \vocab{real vector space}, and if $F = \C$, then we call it a \vocab{complex vector space}.

\begin{example}[Example of a Real Vector Space]
  Let $V$ be the set of all functions $f: [0, 1] \rightarrow \R$ where $f(0) = f(1) = 0$ and $f$ is smooth. Then $V$ is a real vector space.

  Indeed we have
  \begin{align*}
      (f + g)(x) &= f(x) + g(x), \\
      (\lambda f)(x) &= \lambda (f(x)).
  \end{align*}
  The other properties can be checked as required.
\end{example}
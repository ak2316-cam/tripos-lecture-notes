\documentclass[a4paper]{scrartcl}

\usepackage[
    fancytheorems, 
    fancyproofs, 
    noindent, 
]{adam}


\title{Linear Algebra}
\author{Adam Kelly (\texttt{ak2316@cam.ac.uk})}
\date{\today}

\allowdisplaybreaks

\begin{document}

\maketitle

% This is a short description of the course. It should give a little flavour of what the course is about, and what will be roughly covered in the notes.

This article constitutes my notes for the `Linear Algebra' course, held in Michaelmas 2021 at Cambridge. These notes are \emph{not a transcription of the lectures}, and differ significantly in quite a few areas. Still, all lectured material should be covered.



\tableofcontents

\section{Vector Spaces}

\subsection{Vector Spaces and Subspaces}

Linear algebra is, somewhat obviously, primarily about studying objects that are \emph{linear} in nature. The objects we really care about are \emph{vector spaces}, settings in which we can add elements and multiply by scalars. We are also going to consider \emph{linear maps}, functions on vector spaces which preserve that linear structure -- but more on that later. 

Throughout the following discussion (and this course), $\F$ is going to denote an arbitrary field\footnote{A field $\F$ is a set $\F$ equipped with two operations $+$ (`addition') and $\cdot$ (`multiplication'). We require $\F$ with addition to form an abelian group, and multiplication must be associative and have an identity element $1$. We also require every element except $0$ to have an inverse with respect to multiplication, and multiplication must be distributive over addition.

Informally, you can think of a field as something you can do arithmetic in.}

\begin{definition}[$\F$-Vector Space ]
    An \vocab{$\F$-vector space} is an abelian group $(V, +)$ together with a function $\F \times V \rightarrow V$, written $(\lambda, v) \mapsto \lambda v$ such that the following axioms hold:
    \begin{enumerate}[label=(\roman*)]
        \item \emph{Distributivity in $V$}. $\lambda(v_1 + v_2) = \lambda v_1 + \lambda v_2$,
        \item \emph{Distributivity in $\F$}. $(\lambda_1 + \lambda_2)v = \lambda_1 v + \lambda_2 v$,
        \item \emph{Associativity}. $\lambda(\mu v) = (\lambda \mu) v$,
        \item \emph{Identity}. $1v = v$.
    \end{enumerate} 
\end{definition}

We usually call elements of $V$ \vocab{vectors} and elements of $\F$ \vocab{scalars}. The identity element in $V$ is usually called the zero vector, and is written $0_V$ (or just $0$ if the context is clear).

If $\F$ is $\R$ or $\C$, we use the terms `real vector space' and `complex vector space', since they're so common. 


\begin{example}[Examples of Vector Spaces]~
    \vspace{-1.5\baselineskip}
    \begin{enumerate}[label=(\roman*)]
        
        \item The set of triples
        $$
        \{(x, y, z) \mid x, y, z \in \R\}
        $$ 
        forms a real vector space called $\R^3$, because you can add any two triples component wise. 
        \item The set
        $$
        \Q[\sqrt{2}] = \{a + b\sqrt{2} \mid a, b \in \Q\}
        $$
        is a $\Q$-vector space, where we add elements and scale by rational numbers in the obvious way.
        \item The set $\mathcal{C}[0, 1]$ of all continuous functions $f: [0, 1] \rightarrow \R$ forms a real vector space.
    \end{enumerate}
\end{example}

As with many new objects, it's helpful to be able to discuss its substructure. In the case of a vector space $V$, there's a pretty natural notion for what it means for a subset $U \subseteq V$ to still act like a vector space.

\begin{definition}[Subspace]
    Let $V$ be a $\F$-vector space.
    A subset $U \subseteq V$ is a \vocab{subspace} of $V$ if $U$ is also an $\F$-vector space. If $U$ is a subspace of $V$, we will write $U \leq V$.
    
    % \begin{enumerate}[label=(\roman*)]
    %     \item $0 \in U$,
    %     \item $u_1, u_2 \in U$ implies that $u_1 + u_2 \in U$,
    %     \item $\lambda \in \F$, $u \in U$ implies that $\lambda u \in U$.
    % \end{enumerate}
\end{definition}

\begin{example}[Examples of Subspaces]~
    \vspace{-1.5\baselineskip}
    \begin{enumerate}[label=(\roman*)]
        \item The set of vectors $\{(x, y, z) \mid x, y, z \in \R, x + y + z = 0\}$ is a subspace of $\R^3$.
        \item The set of polynomials with terms of even degree $\{ a_0 + a_2 x^2 + a_4 x^4 + \cdots + a_{2k} x^{2k} \mid \alpha_{2i} \in \R, k \in \N\}$ is a subspace of $\R[X]$, the vector space of polynomials with coefficients in $\R$.
    \end{enumerate}
\end{example}

As you would expect, checking that something is a subspace is usually easier than checking all of the axioms for a vector space. In particular, to check that $U$ is a subspace of an $\F$-vector space $V$, you can just check that the following hold:
\begin{itemize}
    \item \emph{Zero vector}\footnote{You may wonder why we need to check this when we already check that we are closed under scaling. To see why, notice that we still have to ensure $U$ is non-empty!}. $0_V \in U$,
    \item \emph{Closure under addition}. $u_1, u_2 \in U$ to imply $u_1 + u_2 \in U$,
    \item \emph{Closure under scaling}. $\lambda \in \F$ and $u \in U$ to imply $\lambda u \in U$.
\end{itemize}


There are various ways in which we can manipulate subspaces, for example we can take the intersection of two subspaces, and we will get back another subspace. 

\begin{proposition}[Intersecting Subspaces]
    Let $U, W \leq V$. Then $U \cap W \leq V$.
\end{proposition}
\begin{proof}
    Since $U$ and $V$ are both subspaces of $V$, we have $0_V \in U \cap V$, and also since they are both closed under addition and scaling, $u_1, u_2 \in U \cap W$ implies that $u_1 + u_2 \in U \cap W$, and $\lambda \in \F$ implies $\lambda u \in U\cap W$. Thus $U \cap W$ is a subspace of $V$. 
\end{proof}

However we can't manipulate subspaces however we want and expect magic. For example, the union of two subspaces is generally \emph{not} a subspace, as it is typically not closed under addition. In fact, the union is only ever a subspace if one of the subspaces is contained in the other.\footnote{There are some more exercises of this flavour on the example sheet.}

We can however try to `complete' the union so that it becomes a subspace.

\begin{definition}[Sum of Subspaces]
    Let $V$ be a vector space over $\F$, and let $U, W \leq V$. We define the \vocab{sum} of $U$ and $W$ to be the set
    $$
    U + W = \{u + w \mid u \in U, w \in W \}.
    $$
\end{definition}

This definition immediately forces $U + W \leq V$, and indeed it is the minimal such space (in that any subspace of $V$ containing both $U$ and $W$ must also contain $U + W$).

\subsection{Quotient Spaces}

Since a vector space $V$ forms an abelian group $(V, +)$, we are able to take the quotient by any subspace $U \leq V$. 

\begin{definition}[Quotient Space]
    Let $V$ an $\F$-vector space, and let $U \leq V$. The \vocab{quotient space} $V/U$ is the abelian group $V/U$ equipped with the scalar multiplication $F \times V/U \rightarrow V/U$ written $(\lambda, v + U) \mapsto \lambda v + U$.
\end{definition}

With this definition, we need to check that this scalar multiplication operation 
is well defined. Indeed, if $v_1 + U = v_2 + U$ then
\begin{align*}
    v_1 - v_2 &\in U \\
\implies \lambda(v_1 - v_2) &\in U \\
\implies \lambda v_1 + U &= \lambda v_2 + U \in V/U,
\end{align*}
so our operation is indeed well defined.

As you would expect, taking a quotient gives you back a vector space.

\begin{proposition}[Quotient Spaces are Vector Spaces]
    $V/U$ is an $\F$-vector space.
\end{proposition}
\begin{proof}[Proof Sketch]
    Check definitions (most properties are inherited from $V$ being a vector space).
\end{proof}

% \subsection{Spans, Linear Independence, and Steinitz Exchange Lemma}

\subsection{Basis and Dimension}

A familiar idea is that of \emph{dimension}, which measures the amount of freedom in a vector space.

\begin{definition}[Span]
    Let $V$ be a vector space over $\F$, and let $S \subset V$. We define the \vocab{span} of $S$, $\langle S \rangle$ to be the set of finite combinations of elements of $S$.
    % TODO: Write this mathematically
\end{definition}

\begin{remark}
    By convention, we also take $\langle \emptyset \rangle = \{ 0\}$.
\end{remark}

    The span of $S$, $\langle S \rangle$ is the smallest vector subspace of $V$ that contains $S$.


\begin{example}
    If $V = \R^3$, and $S = \{(1, 0, 0), (0, 1, 2), (3, -2, -4)\}$, then $\langle S \rangle = \{(a, b, 2b) \mid a, b \in \R\}$.
\end{example}

% \begin{example}
%     Let $X$ be a set, and let $V = \R^X$ be the set of functions $f: X \rightarrow \R$. Then if $\ii_{X}$ is the indicator function for $X$,
%     $$
%     \langle S_X\rangle = \{ \}.
%     $$
%     % TODO: Fix this
% \end{example}

\begin{definition}[Spanning Set]
    Let $V$ be a vector space over $\F$, and let $S \subset V$. We say that $S$ \vocab{spans} $V$ if $\langle S \rangle = V$.
\end{definition}

\begin{definition}[Finite Dimension]
    Let $V$ be a vector space over $\F$. We say that $V$ is \vocab{finite dimensional} if it is spanned by a finite set.
\end{definition}

\begin{example}[Finite and Infinite Dimensional Vector Spaces]
    The space $\R[X]$ of polynomials with coefficients in $\R$ is infinite dimensional, but the space $\R_n[X]$ of polynomials of degree at most $n$ is finite dimensional, and is spanned by the set $\{1, X, X^2, \dots, X^n\}$.
\end{example}

\begin{definition}[Linear Independence]
We say that $\{v_1, \dots, v_n\} \in V$ are \vocab{linearly independent} if
$$
\lambda_1 v_1 + \lambda_2 v_2 + \cdots + \lambda_n v_n = 0
$$
implies that $\lambda_1 = \cdots = \lambda_n = 0$.
\end{definition}

If a set of vectors is \emph{not} linearly independent, then one of the vectors in the set can be written as a linear combination of the others. 

\begin{remark}
    If $\{v_1, \dots, v_n\}$ are linearly independent, then $v_i \neq 0$ for all $i$.
\end{remark}

\begin{definition}[Basis]
    A subset $S$ of $V$ is a \vocab{basis} of $V$ if $S$ is a set of linearly independent vectors that span $V$.
\end{definition}

\begin{remark}
    When $S$ spans $V$, we say that $S$ is a \vocab{generating family} of $V$.
\end{remark}

\begin{example}[Canonical Basis for $\R^n$]
    $\R^n$ has a basis $\{(1, 0, \dots, 0), (0, 1, \dots, 0), \dots, (0, 0, \dots, 1)\}$, which is known as the \vocab{canonical basis}.
\end{example}

\begin{lemma}[Unique Representations with a Basis]
    Let $V$ be a vector space over $\F$. Then $\{v_1, \dots, v_n\}$ is a basis of $V$ if and only if any vector $v \in V$ can be written uniquely as a linear combination of $\{v_1, \dots, v_n\}$.
\end{lemma}
\begin{proof}
    If $v \in V$ can't be written as such a linear combination, then $\{v_1, \dots, v_n\}$ does not span $V$ and is thus not a basis. Also, if $v$ can be written as such a combination but it is non unique, then taking 
    $$
v = \lambda_1 v_1 + \cdots + \lambda_n v_n = \mu_1 v_1 + \cdots + \mu_n v_n.
    $$
    where $\lambda_i \neq \mu_i$ for at least one value of $i$, we'd have
    $0 = v - v = (\lambda_1 - \mu_1) v_1 + \cdots + (\lambda_n - \mu_n)v_n$,
    and at least one of these coefficients must be non-zero, thus $\{v_1, \dots, v_n\}$ is not a linearly independent set.

    Other direction is an exercise.
\end{proof}

\begin{lemma}[Spanning Sets Contain a Basis]
If $\{v_1, \dots, v_n\}$ spans $V$, then some subset of this family is a basis of $V$. 
\end{lemma}
\begin{proof}
    If $\{v_1, \dots, v_n\}$ is linearly independent, then we are done. If it's not, then (up to reordering) we have $v_n \in \langle\{v_1, \dots, v_{n - 1}\}\rangle$. But then $\langle\{v_1, \dots, v_n\}\rangle = \langle\{v_1, \dots, v_{n - 1}\}\rangle$. Removing elements in this way repeatedly, since there is finitely many elements in this set we must eventually get a linearly independent set that still spans $V$.
\end{proof}

\begin{theorem}[Steinitz Exchange Lemma]
    Let $V$ be a finite dimensional vector space over $\F$. Suppose that $\{v_1, \dots, v_m\}$ is a linearly independent set and that $\{w_1, \dots, w_n\}$ spans $V$. Then
    \begin{enumerate}[label=(\roman*)]
        \item $m \leq n$,
        \item Up to reordering $\{v-1, \dots, v_m, w_{m + 1}, \dots, w_n\}$ spans $V$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    Suppose that we've replaced $l$ of the $w_i$,
    and that $\langle v_1, \dots, v_l, w_{l + 1}, \dots, w_n\rangle = V$. If $m = l$ then we are done. Otherwise if $l < m$ then $v_{l + 1} \in V$ implies we can write
    $$
    v_{l + 1} = \sum_{i \leq l} \alpha_i v_i + \sum_{i > l} \beta_i w_i.
    $$
    Since $\{v_1, \dots, v_m\}$ is linearly independent, then one of the $\beta_i$ has to be non-zero. 
    
    Finish after the lecture.
\end{proof}

\end{document}

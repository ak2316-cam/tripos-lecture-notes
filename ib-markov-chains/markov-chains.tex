\documentclass[a4paper]{scrartcl}

\usepackage[
fancytheorems, 
fancyproofs, 
noindent, 
%  spacingfix,  
]{adam}

\usepackage{tikz}
\usetikzlibrary{automata, positioning}

\title{Markov Chains}
\author{Adam Kelly (\texttt{ak2316@cam.ac.uk})}
\date{\today}


\allowdisplaybreaks

\begin{document}

\maketitle

A stochastic process is said to have the `Markov property' if, conditional on its present value, the future is independent of the past.

This is a \emph{restrictive} assumption, but we do end up with a useful model with a rich mathematical theory, which we shall study in this course.

This article constitutes my notes for the `Markov Chains' course, held in Michaelmas 2021 at Cambridge. These notes are \emph{not a transcription of the lectures}, and differ significantly in quite a few areas. Still, all lectured material should be covered.


\tableofcontents

% \clearpage

\section{The Markov Property}

\subsection{What is a Markov Chain?}

Let $S$ be a countable set (the set of possible `states'), and let $X_n$ be a sequence of random variables taking values in $S$.

\begin{definition}[Markov Chain]
	The sequence of random variables $X_n$ is a \vocab{Markov chain} if it satisfies the \vocab{Markov property}
	$$
	\PP(X_{n + 1} = x_{n + 1} \mid X_n = x_n, X_0 = x_0) = \PP(X_{n + 1} = x_{n + 1} \mid X_n = x_n).
	$$

	The Markov chain is said to be \vocab{homogeneous} if for all $i, j \in S$ the conditional probability $\PP(X_{n + 1} = j \mid X_n = i)$ is independent of $n$.
\end{definition}

In this course we are only going to study homogeneous Markov chains.


\section{Introduction}

For this whole course, $I$ will be a finite or countable set. All of our random variables will also be defined on the same probability space $(\Omega, \FF, \PP)$.

\begin{definition}[Markov Chain]
	A \vocab{stochastic process} $(X_n)_{n \geq 0}$ is called a \vocab{Markov chain} if for all $n \geq 0$ and all $x_0, \dots, x_{n+1} \in I$, we have\footnote{We assume here that we are not conditioning on a zero probability event.}
	$$
	\PP(X_{n + 1} = x_{n + 1} \mid X_n = x_n, \dots, X_0 = x_0) = \PP(X_{n + 1} = x_{n + 1} \mid X_n = x_n).
	$$
\end{definition}

\begin{remark}
	This definition gives a \emph{discrete time} Markov chain. It is possible to define a continuous time Markov chain, but we won't worry about that for now.
\end{remark}

If $\PP(X_{n + 1} = y \mid X_n = x)$ for all $x, y \in I$ is independent of $n$, then $X$ is called \vocab{time-homogeneous}. Otherwise, it is \vocab{time-inhomogeneous}. In this course, we will only study time-homogeneous Markov chains.

We will write $P(x, y) = \PP(X_1 = y \mid X_0 = x)$, where $x, y \in I$. We call $P$ a \vocab{stochastic matrix}, because
$$
	\sum_{y \in I} P(x, y) = 1,
$$
that is, the sum of each row is 1.


\begin{remark}
	The index set does not have to be $\N$, it could be say $\{0, 1, \dots, N\}$ for $N \in \N$.
\end{remark}

So to characterize a Markov chain, we need this matrix $P$, giving the probability of passing from a state $x$ to a state $y$. We call this matrix the \vocab{transition matrix} of $X$.

\begin{definition}[Markov]
	We say that $X$ is \vocab{$\markov(\lambda, P)$} if $X_0$ has distribution $\lambda$ and $P$ is the transition matrix. That is,
	\begin{enumerate}[label=(\roman*)]
		\item $\PP(X_0 = x_0) = \lambda_{x_0}$, $x_0 \in I$,
		\item $\PP(X_{n + 1} = x_{n + 1} \mid X_n = x_n, \dots, X_0 = x_0) = P(x_n, x_{n + 1}) = P_{x_n x_{n + 1}}$.
	\end{enumerate}
\end{definition}

We usually represent a Markov chain by its diagram corresponding to the allowed transitions. 

\begin{example}[Diagram of a Markov Chain]
	Let $\alpha, \beta \in (0, 1)$. We consider the matrix
	$$
	P = \begin{pmatrix}
		\alpha & 1 - \alpha \\
		1 - \beta & \beta
	\end{pmatrix}.
	$$
	This is a transition matrix on two states which we can call $1$ and $2$. Here $\alpha$ is the probability of staying at 1, and $1 - \alpha$ is the probability of moving from state 2 when at state 1.

	A diagram of this is given below. This is a directed graph with the relevant probabilities labelling each edge.

	\begin{center}
		\begin{tikzpicture}

			\node[state] (1) {1};
			\node[state, right=of 1] (2) {2};
			
			\draw[every loop, >=latex]
			(1) edge[loop above] node {$\alpha$} (1)
			(1) edge[bend left, auto=left] node {$1 - \alpha$} (2)
			(2) edge[bend left, auto=left] node {$1 - \beta$} (1)
			(2) edge[loop above] node {$\beta$} (2);
			
			\end{tikzpicture}
	\end{center}
\end{example}

\begin{example}
	Suppose that we have the transition matrix
	$$
	P = \begin{pmatrix}
		\frac{1}{2} & \frac{1}{2} & 0 \\
		\frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\
		1 & 0 & 0 
	\end{pmatrix}.
	$$
	This is a transition matrix on three states and corresponds with the diagram below.

	\begin{center}
		\begin{tikzpicture}
		
		\node[state] (1) {1};
		\node [right=of 1] (4) {};
		\node[state, right=of 4] (2) {2};
		\node[state, below=of 4] (3) {3};
		
		\draw[every loop, >=latex]
		(1) edge[loop above] node {$\frac{1}{2}$} (1)
		(1) edge[bend left, auto=left] node {$\frac{1}{2}$} (2)
		(2) edge[bend left, auto=left] node[above] {$\frac{2}{3}$} (1)
		(2) edge[loop above] node {$\frac{1}{3}$} (2)
		(2) edge[bend left, auto=left] node {$\frac{1}{3}$} (3)
		(3) edge[bend left, auto=left] node {$1$} (1);
		
		\end{tikzpicture}
		\end{center}

\end{example}

\begin{theorem}
	The process $X$ is $\markov(\lambda, P)$ if and only if for all $n \geq 0$ and all $x_0, \dots, x_n \in I$ we have
	$$
	\PP(X_0 = x_0, \dots, X_n = x_n) = \lambda_{x_0} P(x_0, x_1) \cdots P(x_{n - 1}, x_n).
	$$
\end{theorem}
\begin{proof}
	First suppose that $X$ is $\markov(\lambda, P)$. Then
	\begin{align*}
		\PP(X_n = x_n, \dots, X_0 = x_0) &= \PP(X_n = x_n \mid X_{n - 1} = x_{n - 1} , \dots, X_0 = x_0) \\&\ \cdot \PP(X_{n - 1} = x_{n - 1} , \dots, X_0 = x_0)\\
		&= P(x_{n - 1}, x_n) \cdot \PP(X_{n - 1} = x_{n - 1} , \dots, X_0 = x_0) \\
		&= P(x_0, x_1) \dots P(x_{n - 1}, x_n) \PP(X_0 = x_0) \\
		&= \lambda_{x_0} P(x_0, x_1) \cdots P(x_{n - 1}, x_n),
	\end{align*}
	as required.

	Now suppose that the property holds. Then $n = 0$ gives $\PP(X_0 = x_0) = \lambda_{x_0}$, so our base case holds. Then
	\begin{align*}
		\PP(X_n = x_)n \mid X_{n - 1} = x_{n - 1} , \dots, X_0 = x_0) &= \frac{\lambda_{x_0}P(x_0, x_1)\cdots P(x_{n - 1}, x_n)}{\lambda_{x_0}P(x_0, x_1)\cdots P(x_{n - 2}, x_{n-1})}\\ 
		&= P(x_{n - 1}, x_n)
	\end{align*}
\end{proof}

Now we are going to define some useful notation.

\begin{definition}[$\delta_i$-mass]
	For $i \in I$, the \vocab{$\delta_i$-mass} of $i$ is defined as
	$\delta_{ij} = \ii(i = j)$.
\end{definition}

Recall the notion of independence for random variables. Let $X_1, \dots, X_n$ be discrete random variables. They are \emph{independent} if for all $x_1, \dots, x_n \in I$, we have
$$
\PP(X_1= x_1, \dots, X_n = x_n) = \prod_{i = 1}^n \PP(X_i = x_i).
$$

We have a similar notion for sequences of random variables. We say a sequence $(X_n)_{n \geq 0}$ is \emph{independent} if for all $i_1 < i_2 < \dots < i_k$ and all $x_1, \dots, x_k$,
$$
\PP(X_{i_1} = x_1, \dots, X_{i_k} = x_k) = \prod_{j = 1}^k \PP(X_{i_j} = x_j).
$$

If $X = (X_n)_{n \geq 0}$ and $Y = (Y_n)_{n \geq 0}$ are two sequences of random variables, they are independent if for all $k, m$ and $i_1 < \cdots < i_k$, $j_1 < \cdots j_m$ we have
\begin{align*}
	&\ \PP(X_{i_1} = x_1, \dots, X_{i_k} = x_k, Y_{j_1} = y_1, \dots, Y_{j_m} = y_m) \\
	&= \PP(X_{i_1} = x_1, \dots, X_{i_k} = x_k) \cdot \PP(Y_{j_1} = y_1, \dots, Y_{j_m} = y_m) 
\end{align*}

\begin{theorem}[Simple Markov Property]
	Suppose that $X \sim \markov(\lambda, P)$. Fix $m \in \N$ and $i \in I$. Conditional on $X_m = i$, the process $(X_{m + n})_{n \geq 0}$ is $\markov(\delta_i, P)$ and it is independent of $X_0, \dots, X_m$.
\end{theorem}

\begin{proof}
	We have
	$$
		\PP\left(X_{m+n}=x_{m+n}, \ldots, X_{m}= x_{m} \mid X_{m}=i\right) = \frac{\PP(X_{m + n} = x_{m + n}, \dots, X_{m} = x_m) \delta_{i x_m}}{\PP(X_m = i)}.
		$$
	We can rewrite the numerator as
	\begin{align*}
	&\mathbb{P}\left(X_{m+n}=x_{m+n}, \ldots, X_{m}=x_{m}\right) \\
=\  &\sum_{x_{0}, \ldots, x_{m-1} \in I} \mathbb{P}\left(X_{m+n}=x_{m+n}, \ldots, X_{m}=x_{m}, X_{m-1}=x_{m-1}, \ldots, X_{0} =x_{0}\right) \\
=\ &\sum_{x_{0}, \cdots, x_{m-1}} \lambda_{x_{0}} P\left(x_{0}, x_{1}\right) \cdots P(x_{m-1}, x_{m}) P(x_{m}, x_{m+1}) \cdots P(x_{m+n-1}, x_{m+n})
 \\
=\ &P\left(x_{m}, x_{m+1}\right) \cdots P\left(x_{m+n-1}, x_{m+n}\right) \mathbb{P}\left(X_{m}=x_{m}\right).
	\end{align*}
	Substituting this back into our original expression, we get 
	$$
	\PP\left(X_{m+n}=x_{m+n}, \ldots, X_{m}= x_{m} \mid X_{m}=i\right) = \delta_{i x_m} P(x_m, x_{m + 1}) \cdots P(x_{m + n - 1}, x_{m + n}),
	$$
	showing that $(X_{m + n})_{n \geq 0}$ is $\markov(\delta_i, P)$ conditional on $X_m = i$.
\end{proof}

\begin{remark}
	Informally, this theorem says `past and future are independent given the present'.
\end{remark}

\section{MISSING LECTURE 2}

\section{Lecture 3}

\begin{example}
	Given the transition matrix
	$$
	P = \begin{pmatrix}
		0 & 1 & 0 \\
		0 & 1/2 & 1/2 \\ 
		1/2 & 0 & 1/2
	\end{pmatrix},
	$$
	we want to find $p_{11}(n)$. The eigenvalues of this matrix are $1, i/2$ and $-i/2$. We write $i/2 = (\cos \pi/2 + i \sin \pi/2)/2$, and then we can write the general form for $p_{11}(n)$ as
	$$
	p_{11}(n) = \alpha + \beta \cdot \left(\frac{1}{2}\right)^n \cos \left(\frac{n \pi}{2}\right) + \gamma \cdot \left(\frac{1}{2}\right)^n \sin\left(\frac{n \pi}{2}\right).
	$$
	We can then compute by hand $p_{11}(0) = 1$, $p_{11}(1) = 0$ and $p_{11}(2) = 0$. So, if you solve this system for $\alpha, \beta, \gamma$, you get
	$$
	p_{11}(n) = \frac{1}{5} + \left(\frac{1}{2}\right)^n \left(\frac{4}{5}\cos\left(\frac{n \pi}{2}\right) - \frac{2}{5} \sin \left(\frac{n \pi}{2}\right)\right).
	$$
\end{example}

\subsection{Communicating Classes}

\begin{definition}
$X$ is a Markov chain with transition matrix $P$ and values in $I$. For $x, y \in I$ we say that \vocab{$x$ leads to $y$} and write it $x \rightarrow y$ if
$$
\PP(X_m = y \text{ for some }n \geq 0) > 0. 
$$
We say that $x$ \vocab{communicates} with $y$ and write $x \longleftrightarrow y$ if both $x \rightarrow y$ and $y \rightarrow x$.  
\end{definition}

\begin{theorem}
	The following are equivalent: 
	\begin{enumerate}[label=(\roman*)]
		\item $x \rightarrow y$;
		\item There exists a sequence of states $x = x_0, x_1, \dots, x_k = y$ such that 
		$$P(x_0, x_1)P(x_1, x_2) \cdots P(x_{k - 1}, x_k) > 0;$$
		\item There exists $n \geq 0$ such that $p_{xy}(n) > 0$.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Trivial.
\end{proof}

\begin{corollary}
	$\longleftrightarrow$ is an equivalence relation on $I$.
\end{corollary}
\begin{proof}
	Trivial.
\end{proof}

\begin{definition}[Communicating Classes]
	The equivalence classes induced by $\longleftrightarrow$ on $I$ are called \vocab{communicating classes}.
\end{definition}

A communicating class $C$ is \vocab{closed} if whenever $x \in C$ and $x \rightarrow y$ then $y \in C$.

A matrix $P$ is called \vocab{irreducible} if it has a single communicating class, that is, for all $x, y \in I$ we have $x \longleftrightarrow y$. 

A state $x$ is called \vocab{absorbing} if $\{x\}$ is a closed class. 

\subsection{Hitting Times}

\begin{definition}
	For $A \subseteq I$, we define $T_A$ to be the \vocab{hitting time} of $A$,
	$
	T_A: \Omega \rightarrow \{0, 1, 2, \dots \} \cup \{\infty\},
	$
	defined by $T_A(\omega) = \inf\{ n \geq 0: X_n(\omega) \in A\}$, where we take $\inf \emptyset = \infty$.

	The \vocab{hitting probability} of $A$ is $h^A: I \rightarrow [0, 1]$ such that $h_i^A = \PP_i(T_A < \infty)$.

	The \vocab{mean hitting time} of $A$ is $k^A: I \rightarrow \R$ with $k_i^A = \EE_i[T_A] = \sum_{n = 0}^{\infty} n \cdot \PP_i(T_a = n) + \infty \cdot \PP_i(T_a = \infty)$.
\end{definition}

\begin{example}
	Consider the Markov chain in the diagram below.
	\begin{center}
		\begin{tikzpicture}
		
		\node[state] (1) {1};
		\node[state, right=of 1] (2) {2};
		\node[state, right=of 2] (3) {3};
		\node[state, right=of 3] (4) {4};
		
		\draw[every loop, >=latex]
		% (1) edge[loop above] node {1} (1)
		(2) edge[auto=left] node {$\frac{1}{2}$} (1)
		(2) edge[bend left, auto=left] node {$\frac{1}{2}$} (3)
		(3) edge[bend left, auto=left] node {$\frac{1}{2}$} (2)
		(3) edge[auto=left] node {$\frac{1}{2}$} (4);
		% (4) edge[loop above] node {1} (4);
		
		\end{tikzpicture}
		\end{center}

		We take $A = \{4\}$, and want to find $h_2^A = \PP_2(T_A < \infty)$. We have
		\begin{align*}
			h_2^A &= \frac{1}{2}h_3^A \\
			h_3^A &= \frac{1}{2} \cdot 1 + \frac{1}{2} h_2^A \\
		\implies h_2^A &= \frac{1}{3}.
		\end{align*}
		If instead we took $B = \{1, 4\}$ and wanted to find $k_2^B$, we would get
		\begin{align*}
			k_2^B &= 1 + \frac{1}{2}k_3^B \\
			k_3^B &= 1 + \frac{1}{2}k_2^B \\
	\implies k_2^B &= 2.
		\end{align*}
\end{example}

In the computations above, we really should check that this is a valid method (though it is quite intuitive).

\begin{theorem}
	Let $A \subseteq I$. The vector $(h_i^A)_{i \in A}$ is the minimal non-negative solution to
	\begin{align*}
		h_i^A = \begin{cases}
			1 &\mbox{if } i  \in A, \\
			\sum_{j} P(i, j) h_j^A &\mbox{if } i \not \in A,
		   \end{cases}
	\end{align*}
	where minimality means that if $(x_i)_{i \in A}$ is another solution to the linear system, then $x_i \geq h_i^A$ for all $i$.
\end{theorem}
\begin{proof}
	We first check that $h_i$ does indeed solve this system. 
\end{proof}

\end{document}

